<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns:z="http://www.zotero.org/namespaces/export#"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:foaf="http://xmlns.com/foaf/0.1/"
 xmlns:bib="http://purl.org/net/biblio#"
 xmlns:dcterms="http://purl.org/dc/terms/"
 xmlns:link="http://purl.org/rss/1.0/modules/link/"
 xmlns:prism="http://prismstandard.org/namespaces/1.2/basic/"
 xmlns:vcard="http://nwalsh.com/rdf/vCard#">
    <rdf:Description rdf:about="http://arxiv.org/abs/2106.03844">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reiss</foaf:surname>
                        <foaf:givenName>Tal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoshen</foaf:surname>
                        <foaf:givenName>Yedid</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_30"/>
        <link:link rdf:resource="#item_3"/>
        <dc:relation rdf:resource="https://ieeexplore.ieee.org/document/9577693/"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Mean-Shifted Contrastive Loss for Anomaly Detection</dc:title>
        <dcterms:abstract>Deep anomaly detection methods learn representations that separate between normal and anomalous samples. Very effective representations are obtained when powerful externally trained feature extractors (e.g. ResNets pre-trained on ImageNet) are ﬁne-tuned on the training data which consists of normal samples and no anomalies. However, this is a difﬁcult task that can suffer from catastrophic collapse, i.e. it is prone to learning trivial and non-speciﬁc features. In this paper, we propose a new loss function which can overcome failure modes of both center-loss and contrastive-loss methods. Furthermore, we combine it with a conﬁdence-invariant angular center loss, which replaces the Euclidean distance used in previous work, that was sensitive to prediction conﬁdence. Our improvements yield a new anomaly detection approach, based on Mean-Shifted Contrastive Loss, which is both more accurate and less sensitive to catastrophic collapse than previous methods. Our method achieves state-of-the-art anomaly detection performance on multiple benchmarks including 97.5% ROC-AUC on the CIFAR-10 dataset1.</dcterms:abstract>
        <dc:date>2022-11-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2106.03844</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:30:56</dcterms:dateSubmitted>
        <dc:description>arXiv:2106.03844 [cs]</dc:description>
        <prism:number>arXiv:2106.03844</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_30">
       <rdf:value>Comment: AAAI 2023</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_3">
        <z:itemType>attachment</z:itemType>
        <dc:title>Reiss 和 Hoshen - 2022 - Mean-Shifted Contrastive Loss for Anomaly Detectio.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2103.04257">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Guodong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Shumin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Errui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Di</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_32"/>
        <link:link rdf:resource="#item_4"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Student-Teacher Feature Pyramid Matching for Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection is a challenging task and usually formulated as an one-class learning problem for the unexpectedness of anomalies. This paper proposes a simple yet powerful approach to this issue, which is implemented in the student-teacher framework for its advantages but substantially extends it in terms of both accuracy and efﬁciency. Given a strong model pre-trained on image classiﬁcation as the teacher, we distill the knowledge into a single student network with the identical architecture to learn the distribution of anomaly-free images and this one-step transfer preserves the crucial clues as much as possible. Moreover, we integrate the multi-scale feature matching strategy into the framework, and this hierarchical feature matching enables the student network to receive a mixture of multi-level knowledge from the feature pyramid under better supervision, thus allowing to detect anomalies of various sizes. The difference between feature pyramids generated by the two networks serves as a scoring function indicating the probability of anomaly occurring. Due to such operations, our approach achieves accurate and fast pixel-level anomaly detection. Very competitive results are delivered on the MVTec anomaly detection dataset, superior to the state of the art ones.</dcterms:abstract>
        <dc:date>2021-10-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2103.04257</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:30:59</dcterms:dateSubmitted>
        <dc:description>arXiv:2103.04257 [cs]</dc:description>
        <prism:number>arXiv:2103.04257</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_32">
       <rdf:value>Comment: Accepted by BMVC'2021</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_4">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wang 等 - 2021 - Student-Teacher Feature Pyramid Matching for Anoma.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-66544-509-2">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66544-509-2</dc:identifier>
                <dc:title>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR46437.2021.01466</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Nashville, TN, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Salehi</foaf:surname>
                        <foaf:givenName>Mohammadreza</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sadjadi</foaf:surname>
                        <foaf:givenName>Niousha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Baselizadeh</foaf:surname>
                        <foaf:givenName>Soroosh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rohban</foaf:surname>
                        <foaf:givenName>Mohammad H.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rabiee</foaf:surname>
                        <foaf:givenName>Hamid R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_5"/>
        <dc:title>Multiresolution Knowledge Distillation for Anomaly Detection</dc:title>
        <dcterms:abstract>Unsupervised representation learning has proved to be a critical component of anomaly detection/localization in images. The challenges to learn such a representation are two-fold. Firstly, the sample size is not often large enough to learn a rich generalizable representation through conventional techniques. Secondly, while only normal samples are available at training, the learned features should be discriminative of normal and anomalous samples. Here, we propose to use the “distillation” of features at various layers of an expert network, which is pre-trained on ImageNet, into a simpler cloner network to tackle both issues. We detect and localize anomalies using the discrepancy between the expert and cloner networks’ intermediate activation values given an input sample. We show that considering multiple intermediate hints in distillation leads to better exploitation of the expert’s knowledge and a more distinctive discrepancy between the two networks, compared to utilizing only the last layer activation values. Notably, previous methods either fail in precise anomaly localization or need expensive region-based training. In contrast, with no need for any special or intensive training procedure, we incorporate interpretability algorithms in our novel framework to localize anomalous regions. Despite the striking difference between some test datasets and ImageNet, we achieve competitive or signiﬁcantly superior results compared to SOTA on MNIST, F-MNIST, CIFAR-10, MVTecAD, Retinal-OCT, and two other medical datasets on both anomaly detection and localization.</dcterms:abstract>
        <dc:date>6/2021</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9577330/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:04</dcterms:dateSubmitted>
        <bib:pages>14897-14907</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_5">
        <z:itemType>attachment</z:itemType>
        <dc:title>Salehi 等 - 2021 - Multiresolution Knowledge Distillation for Anomaly.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9577693/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66544-509-2</dc:identifier>
                <dc:title>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR46437.2021.00283</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Nashville, TN, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reiss</foaf:surname>
                        <foaf:givenName>Tal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohen</foaf:surname>
                        <foaf:givenName>Niv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergman</foaf:surname>
                        <foaf:givenName>Liron</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoshen</foaf:surname>
                        <foaf:givenName>Yedid</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_6"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/2106.03844"/>
        <dc:title>PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</dc:title>
        <dcterms:abstract>Anomaly detection methods require high-quality features. In recent years, the anomaly detection community has attempted to obtain better features using advances in deep self-supervised feature learning. Surprisingly, a very promising direction, using pre-trained deep features, has been mostly overlooked. In this paper, we ﬁrst empirically establish the perhaps expected, but unreported result, that combining pre-trained features with simple anomaly detection and segmentation methods convincingly outperforms, much more complex, state-of-the-art methods.</dcterms:abstract>
        <dc:date>6/2021</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PANDA</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9577693/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:07</dcterms:dateSubmitted>
        <bib:pages>2805-2813</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_6">
        <z:itemType>attachment</z:itemType>
        <dc:title>Reiss 等 - 2021 - PANDA Adapting Pretrained Features for Anomaly De.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2104.04015">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Chun-Liang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sohn</foaf:surname>
                        <foaf:givenName>Kihyuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>Jinsung</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pfister</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_36"/>
        <link:link rdf:resource="#item_7"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</dc:title>
        <dcterms:abstract>We aim at constructing a high performance model for defect detection that detects unknown anomalous patterns of an image without anomalous data. To this end, we propose a two-stage framework for building anomaly detectors using normal training data only. We ﬁrst learn self-supervised deep representations and then build a generative one-class classiﬁer on learned representations. We learn representations by classifying normal data from the CutPaste, a simple data augmentation strategy that cuts an image patch and pastes at a random location of a large image. Our empirical study on MVTec anomaly detection dataset demonstrates the proposed algorithm is general to be able to detect various types of real-world defects. We bring the improvement upon previous arts by 3.1 AUCs when learning representations from scratch. By transfer learning on pretrained representations on ImageNet, we achieve a new state-of-theart 96.6 AUC. Lastly, we extend the framework to learn and extract representations from patches to allow localizing defective areas without annotations during training.</dcterms:abstract>
        <dc:date>2021-04-08</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CutPaste</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2104.04015</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:12</dcterms:dateSubmitted>
        <dc:description>arXiv:2104.04015 [cs]</dc:description>
        <prism:number>arXiv:2104.04015</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_36">
        <rdf:value>Comment: Published at CVPR 2021. The first two authors contributed equally</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_7">
        <z:itemType>attachment</z:itemType>
        <dc:title>Li 等 - 2021 - CutPaste Self-Supervised Learning for Anomaly Det.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2108.07610">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zavrtanik</foaf:surname>
                        <foaf:givenName>Vitjan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kristan</foaf:surname>
                        <foaf:givenName>Matej</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Skočaj</foaf:surname>
                        <foaf:givenName>Danijel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_45"/>
        <link:link rdf:resource="#item_8"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>DRAEM -- A discriminatively trained reconstruction embedding for surface anomaly detection</dc:title>
        <dcterms:abstract>Visual surface anomaly detection aims to detect local image regions that signiﬁcantly deviate from normal appearance. Recent surface anomaly detection methods rely on generative models to accurately reconstruct the normal areas and to fail on anomalies. These methods are trained only on anomaly-free images, and often require hand-crafted post-processing steps to localize the anomalies, which prohibits optimizing the feature extraction for maximal detection capability. In addition to reconstructive approach, we cast surface anomaly detection primarily as a discriminative problem and propose a discriminatively trained reconstruction anomaly embedding model (DRÆM). The proposed method learns a joint representation of an anomalous image and its anomaly-free reconstruction, while simultaneously learning a decision boundary between normal and anomalous examples. The method enables direct anomaly localization without the need for additional complicated post-processing of the network output and can be trained using simple and general anomaly simulations. On the challenging MVTec anomaly detection dataset, DRÆM outperforms the current state-of-theart unsupervised methods by a large margin and even delivers detection performance close to the fully-supervised methods on the widely used DAGM surface-defect detection dataset, while substantially outperforming them in localization accuracy.</dcterms:abstract>
        <dc:date>2021-09-27</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2108.07610</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:15</dcterms:dateSubmitted>
        <dc:description>arXiv:2108.07610 [cs]</dc:description>
        <prism:number>arXiv:2108.07610</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_45">
       <rdf:value>Comment: Accepted to ICCV2021</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_8">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zavrtanik 等 - 2021 - DRAEM -- A discriminatively trained reconstruction.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2005.14140">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rippel</foaf:surname>
                        <foaf:givenName>Oliver</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mertens</foaf:surname>
                        <foaf:givenName>Patrick</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Merhof</foaf:surname>
                        <foaf:givenName>Dorit</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_52"/>
        <link:link rdf:resource="#item_9"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Modeling the Distribution of Normal Data in Pre-Trained Deep Features for Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly Detection (AD) in images is a fundamental computer vision problem and refers to identifying images and/or image substructures that deviate signiﬁcantly from the norm. Popular AD algorithms commonly try to learn a model of normality from scratch using task speciﬁc datasets, but are limited to semi-supervised approaches employing mostly normal data due to the inaccessibility of anomalies on a large scale combined with the ambiguous nature of anomaly appearance.</dcterms:abstract>
        <dc:date>2020-10-23</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.14140</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:19</dcterms:dateSubmitted>
        <dc:description>arXiv:2005.14140 [cs]</dc:description>
        <prism:number>arXiv:2005.14140</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_52">
        <rdf:value>Comment: Camera-ready for ICPR2020 (8 pages + 4 pages appendix). First two authors contributed equally to this work</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_9">
        <z:itemType>attachment</z:itemType>
        <dc:title>Rippel 等 - 2020 - Modeling the Distribution of Normal Data in Pre-Tr.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2011.08785">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Defard</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Setkov</foaf:surname>
                        <foaf:givenName>Aleksandr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Loesch</foaf:surname>
                        <foaf:givenName>Angelique</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Audigier</foaf:surname>
                        <foaf:givenName>Romaric</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_54"/>
        <link:link rdf:resource="#item_10"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</dc:title>
        <dcterms:abstract>We present a new framework for Patch Distribution Modeling, PaDiM, to concurrently detect and localize anomalies in images in a one-class learning setting. PaDiM makes use of a pretrained convolutional neural network (CNN) for patch embedding, and of multivariate Gaussian distributions to get a probabilistic representation of the normal class. It also exploits correlations between the different semantic levels of CNN to better localize anomalies. PaDiM outperforms current state-ofthe-art approaches for both anomaly detection and localization on the MVTec AD and STC datasets. To match real-world visual industrial inspection, we extend the evaluation protocol to assess performance of anomaly localization algorithms on non-aligned dataset. The state-of-the-art performance and low complexity of PaDiM make it a good candidate for many industrial applications.</dcterms:abstract>
        <dc:date>2020-11-17</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>PaDiM</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2011.08785</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:23</dcterms:dateSubmitted>
        <dc:description>arXiv:2011.08785 [cs]</dc:description>
        <prism:number>arXiv:2011.08785</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_54">
        <rdf:value>Comment: 7 pages, 2 figures, 8 tables, accepted at the 1st International Workshop on Industrial Machine Learning, ICPR 2020</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_10">
        <z:itemType>attachment</z:itemType>
        <dc:title>Defard 等 - 2020 - PaDiM a Patch Distribution Modeling Framework for.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-66540-477-8">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66540-477-8</dc:identifier>
                <dc:title>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
                <dc:identifier>DOI 10.1109/WACV48630.2021.00195</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Waikoloa, HI, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rudolph</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wandt</foaf:surname>
                        <foaf:givenName>Bastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosenhahn</foaf:surname>
                        <foaf:givenName>Bodo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_467"/>
        <link:link rdf:resource="#item_11"/>
        <dc:relation rdf:resource="http://arxiv.org/abs/1907.02392"/>
        <dc:subject>标准化流(Normalizing Flow)</dc:subject>
        <dc:subject>流模型</dc:subject>
        <dc:title>Same Same But DifferNet: Semi-Supervised Defect Detection with Normalizing Flows</dc:title>
        <dcterms:abstract>The detection of manufacturing errors is crucial in fabrication processes to ensure product quality and safety standards. Since many defects occur very rarely and their characteristics are mostly unknown a priori, their detection is still an open research question. To this end, we propose DifferNet: It leverages the descriptiveness of features extracted by convolutional neural networks to estimate their density using normalizing ﬂows. Normalizing ﬂows are well-suited to deal with low dimensional data distributions. However, they struggle with the high dimensionality of images. Therefore, we employ a multi-scale feature extractor which enables the normalizing ﬂow to assign meaningful likelihoods to the images. Based on these likelihoods we develop a scoring function that indicates defects. Moreover, propagating the score back to the image enables pixel-wise localization. To achieve a high robustness and performance we exploit multiple transformations in training and evaluation. In contrast to most other methods, ours does not require a large number of training samples and performs well with as low as 16 images. We demonstrate the superior performance over existing approaches on the challenging and newly proposed MVTec AD [4] and Magnetic Tile Defects [14] datasets.</dcterms:abstract>
        <dc:date>1/2021</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Same Same But DifferNet</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9423203/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:26</dcterms:dateSubmitted>
        <bib:pages>1906-1915</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2021 IEEE Winter Conference on Applications of Computer Vision (WACV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <bib:Memo rdf:about="#item_467">
        <rdf:value>&lt;div data-citation-items=&quot;%5B%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2F11835615%2Fitems%2FZ7VAI79C%22%5D%2C%22itemData%22%3A%7B%22id%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2F11835615%2Fitems%2FZ7VAI79C%22%2C%22type%22%3A%22paper-conference%22%2C%22abstract%22%3A%22The%20detection%20of%20manufacturing%20errors%20is%20crucial%20in%20fabrication%20processes%20to%20ensure%20product%20quality%20and%20safety%20standards.%20Since%20many%20defects%20occur%20very%20rarely%20and%20their%20characteristics%20are%20mostly%20unknown%20a%20priori%2C%20their%20detection%20is%20still%20an%20open%20research%20question.%20To%20this%20end%2C%20we%20propose%20DifferNet%3A%20It%20leverages%20the%20descriptiveness%20of%20features%20extracted%20by%20convolutional%20neural%20networks%20to%20estimate%20their%20density%20using%20normalizing%20%EF%AC%82ows.%20Normalizing%20%EF%AC%82ows%20are%20well-suited%20to%20deal%20with%20low%20dimensional%20data%20distributions.%20However%2C%20they%20struggle%20with%20the%20high%20dimensionality%20of%20images.%20Therefore%2C%20we%20employ%20a%20multi-scale%20feature%20extractor%20which%20enables%20the%20normalizing%20%EF%AC%82ow%20to%20assign%20meaningful%20likelihoods%20to%20the%20images.%20Based%20on%20these%20likelihoods%20we%20develop%20a%20scoring%20function%20that%20indicates%20defects.%20Moreover%2C%20propagating%20the%20score%20back%20to%20the%20image%20enables%20pixel-wise%20localization.%20To%20achieve%20a%20high%20robustness%20and%20performance%20we%20exploit%20multiple%20transformations%20in%20training%20and%20evaluation.%20In%20contrast%20to%20most%20other%20methods%2C%20ours%20does%20not%20require%20a%20large%20number%20of%20training%20samples%20and%20performs%20well%20with%20as%20low%20as%2016%20images.%20We%20demonstrate%20the%20superior%20performance%20over%20existing%20approaches%20on%20the%20challenging%20and%20newly%20proposed%20MVTec%20AD%20%5B4%5D%20and%20Magnetic%20Tile%20Defects%20%5B14%5D%20datasets.%22%2C%22container-title%22%3A%222021%20IEEE%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%20(WACV)%22%2C%22DOI%22%3A%2210.1109%2FWACV48630.2021.00195%22%2C%22event-place%22%3A%22Waikoloa%2C%20HI%2C%20USA%22%2C%22event-title%22%3A%222021%20IEEE%20Winter%20Conference%20on%20Applications%20of%20Computer%20Vision%20(WACV)%22%2C%22ISBN%22%3A%22978-1-66540-477-8%22%2C%22language%22%3A%22en%22%2C%22page%22%3A%221906-1915%22%2C%22publisher%22%3A%22IEEE%22%2C%22publisher-place%22%3A%22Waikoloa%2C%20HI%2C%20USA%22%2C%22source%22%3A%22DOI.org%20(Crossref)%22%2C%22title%22%3A%22Same%20Same%20But%20DifferNet%3A%20Semi-Supervised%20Defect%20Detection%20with%20Normalizing%20Flows%22%2C%22title-short%22%3A%22Same%20Same%20But%20DifferNet%22%2C%22URL%22%3A%22https%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F9423203%2F%22%2C%22author%22%3A%5B%7B%22family%22%3A%22Rudolph%22%2C%22given%22%3A%22Marco%22%7D%2C%7B%22family%22%3A%22Wandt%22%2C%22given%22%3A%22Bastian%22%7D%2C%7B%22family%22%3A%22Rosenhahn%22%2C%22given%22%3A%22Bodo%22%7D%5D%2C%22accessed%22%3A%7B%22date-parts%22%3A%5B%5B%222023%22%2C6%2C4%5D%5D%7D%2C%22issued%22%3A%7B%22date-parts%22%3A%5B%5B%222021%22%2C1%5D%5D%7D%7D%7D%5D&quot; data-schema-version=&quot;8&quot;&gt;&lt;p&gt;文章通过建立正常图片特征向量所在分布与标准正态分布之间的流模型双射关系，将输入图片特征向量变换到一个标准正态分布的隐空间上，根据输入图片的特征向量在隐空间上的概率密度值，确定该图片的异常分数。对于异常图片，通过该图片在隐空间上的特征向量，在模型网络上反向传播得到梯度图，再加以高斯模糊，即可进行异常定位。&lt;/p&gt;
&lt;p&gt;文章创新点：是第一个将流模型应用到缺陷检测中的方法，利用标准化流找将一类正常样本的特征向量映射到标准正态分布上，以找出这类正常样本的分布&lt;/p&gt;
&lt;p&gt;Referred in &lt;a href=&quot;zotero://note/u/XBWUJK7D/?ignore=1&amp;amp;line=5&quot; rel=&quot;noopener noreferrer nofollow&quot;&gt;Distribution Map&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_11">
        <z:itemType>attachment</z:itemType>
        <dc:title>Rudolph 等 - 2021 - Same Same But DifferNet Semi-Supervised Defect De.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://ojs.aaai.org/index.php/AAAI/article/view/19915">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:2374-3468,%202159-5399"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Yuanhong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tian</foaf:surname>
                        <foaf:givenName>Yu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pang</foaf:surname>
                        <foaf:givenName>Guansong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Carneiro</foaf:surname>
                        <foaf:givenName>Gustavo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_12"/>
        <dc:title>Deep One-Class Classification via Interpolated Gaussian Descriptor</dc:title>
        <dcterms:abstract>One-class classification (OCC) aims to learn an effective data description to enclose all normal training samples and detect anomalies based on the deviation from the data description. Current state-of-the-art OCC models learn a compact normality description by hyper-sphere minimisation, but they often suffer from overfitting the training data, especially when the training set is small or contaminated with anomalous samples. To address this issue, we introduce the interpolated Gaussian descriptor (IGD) method, a novel OCC model that learns a one-class Gaussian anomaly classifier trained with adversarially interpolated training samples. The Gaussian anomaly classifier differentiates the training samples based on their distance to the Gaussian centre and the standard deviation of these distances, offering the model a discriminability w.r.t. the given samples during training. The adversarial interpolation is enforced to consistently learn a smooth Gaussian descriptor, even when the training data is small or contaminated with anomalous samples. This enables our model to learn the data description based on the representative normal samples rather than fringe or anomalous samples, resulting in significantly improved normality description. In extensive experiments on diverse popular benchmarks, including MNIST, Fashion MNIST, CIFAR10, MVTec AD and two medical datasets, IGD achieves better detection accuracy than current state-of-the-art models. IGD also shows better robustness in problems with small or contaminated training sets.</dcterms:abstract>
        <dc:date>2022-06-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ojs.aaai.org/index.php/AAAI/article/view/19915</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:30</dcterms:dateSubmitted>
        <bib:pages>383-392</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:2374-3468,%202159-5399">
        <prism:volume>36</prism:volume>
        <dc:title>Proceedings of the AAAI Conference on Artificial Intelligence</dc:title>
        <dc:identifier>DOI 10.1609/aaai.v36i1.19915</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>AAAI</dcterms:alternative>
        <dc:identifier>ISSN 2374-3468, 2159-5399</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_12">
        <z:itemType>attachment</z:itemType>
        <dc:title>Chen 等 - 2022 - Deep One-Class Classification via Interpolated Gau.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2203.00259">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liang</foaf:surname>
                        <foaf:givenName>Yufei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Jiangning</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Shiwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Runze</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Yong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pan</foaf:surname>
                        <foaf:givenName>Shuwen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_13"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection</dc:title>
        <dcterms:abstract>Density-based and classiﬁcation-based methods have ruled unsupervised anomaly detection in recent years, while reconstruction-based methods are rarely mentioned for the poor reconstruction ability and low performance. However, the latter requires no costly extra training samples for the unsupervised training that is more practical, so this paper focuses on improving this kind of method and proposes a novel Omni-frequency Channel-selection Reconstruction (OCR-GAN) network to handle anomaly detection task in a perspective of frequency. Concretely, we propose a Frequency Decoupling (FD) module to decouple the input image into different frequency components and model the reconstruction process as a combination of parallel omnifrequency image restorations, as we observe a signiﬁcant difference in the frequency distribution of normal and abnormal images. Given the correlation among multiple frequencies, we further propose a Channel Selection (CS) module that performs frequency interaction among different encoders by adaptively selecting different channels. Abundant experiments demonstrate the effectiveness and superiority of our approach over different kinds of methods, e.g., achieving a new state-of-the-art 98.3 detection AUC on the MVTec AD dataset without extra training data that markedly surpasses the reconstruction-based baseline by +38.1↑ and the current SOTA method by +0.3↑. Source code will be available at https://github.com/zhangzjn/OCR-GAN.</dcterms:abstract>
        <dc:date>2022-03-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2203.00259</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:34</dcterms:dateSubmitted>
        <dc:description>arXiv:2203.00259 [cs]</dc:description>
        <prism:number>arXiv:2203.00259</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_13">
        <z:itemType>attachment</z:itemType>
        <dc:title>Liang 等 - 2022 - Omni-frequency Channel-selection Representations f.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2110.04538">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Ye</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Rui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bao</foaf:surname>
                        <foaf:givenName>Tianpeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Rui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Liwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_59"/>
        <link:link rdf:resource="#item_14"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization</dc:title>
        <dcterms:abstract>The essence of unsupervised anomaly detection is to learn the compact distribution of normal samples and detect outliers as anomalies in testing. Meanwhile, the anomalies in real-world are usually subtle and ﬁne-grained in a high-resolution image especially for industrial applications. Towards this end, we propose a novel framework for unsupervised anomaly detection and localization. Our method aims at learning dense and compact distribution from normal images with a coarse-to-ﬁne alignment process. The coarse alignment stage standardizes the pixel-wise position of objects in both image and feature levels. The ﬁne alignment stage then densely maximizes the similarity of features among all corresponding locations in a batch. To facilitate the learning with only normal images, we propose a new pretext task called non-contrastive learning for the ﬁne alignment stage. Non-contrastive learning extracts robust and discriminating normal image representations without making assumptions on abnormal samples, and it thus empowers our model to generalize to various anomalous scenarios. Extensive experiments on two typical industrial datasets of MVTec AD and BenTech AD demonstrate that our framework is effective in detecting various real-world defects and achieves a new state-of-the-art in industrial unsupervised anomaly detection.</dcterms:abstract>
        <dc:date>2022-07-18</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Focus Your Distribution</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2110.04538</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:38</dcterms:dateSubmitted>
        <dc:description>arXiv:2110.04538 [cs]</dc:description>
        <prism:number>arXiv:2110.04538</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_59">
       <rdf:value>Comment: ICME2022 oral</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_14">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zheng 等 - 2022 - Focus Your Distribution Coarse-to-Fine Non-Contra.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-66546-946-3">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66546-946-3</dc:identifier>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR52688.2022.00951</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New Orleans, LA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Deng</foaf:surname>
                        <foaf:givenName>Hanqiu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Xingyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_15"/>
        <dc:title>Anomaly Detection via Reverse Distillation from One-Class Embedding</dc:title>
        <dcterms:abstract>Knowledge distillation (KD) achieves promising results on the challenging problem of unsupervised anomaly detection (AD). The representation discrepancy of anomalies in the teacher-student (T-S) model provides essential evidence for AD. However, using similar or identical architectures to build the teacher and student models in previous studies hinders the diversity of anomalous representations. To tackle this problem, we propose a novel T-S model consisting of a teacher encoder and a student decoder and introduce a simple yet effective ”reverse distillation” paradigm accordingly. Instead of receiving raw images directly, the student network takes teacher model’s one-class embedding as input and targets to restore the teacher’s multiscale representations. Inherently, knowledge distillation in this study starts from abstract, high-level presentations to low-level features. In addition, we introduce a trainable one-class bottleneck embedding (OCBE) module in our T-S model. The obtained compact embedding effectively preserves essential information on normal patterns, but abandons anomaly perturbations. Extensive experimentation on AD and one-class novelty detection benchmarks shows that our method surpasses SOTA performance, demonstrating our proposed approach’s effectiveness and generalizability.</dcterms:abstract>
        <dc:date>6/2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9879956/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:42</dcterms:dateSubmitted>
        <bib:pages>9727-9736</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_15">
        <z:itemType>attachment</z:itemType>
        <dc:title>Deng 和 Li - 2022 - Anomaly Detection via Reverse Distillation from On.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9880272/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66546-946-3</dc:identifier>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR52688.2022.01321</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New Orleans, LA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ristea</foaf:surname>
                        <foaf:givenName>Nicolae-Catalin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Madan</foaf:surname>
                        <foaf:givenName>Neelu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ionescu</foaf:surname>
                        <foaf:givenName>Radu Tudor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nasrollahi</foaf:surname>
                        <foaf:givenName>Kamal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Khan</foaf:surname>
                        <foaf:givenName>Fahad Shahbaz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Moeslund</foaf:surname>
                        <foaf:givenName>Thomas B.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shah</foaf:surname>
                        <foaf:givenName>Mubarak</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_16"/>
        <dc:title>Self-Supervised Predictive Convolutional Attentive Block for Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection is commonly pursued as a one-class classiﬁcation problem, where models can only learn from normal training samples, while being evaluated on both normal and abnormal test samples. Among the successful approaches for anomaly detection, a distinguished category of methods relies on predicting masked information (e.g. patches, future frames, etc.) and leveraging the reconstruction error with respect to the masked information as an abnormality score. Different from related methods, we propose to integrate the reconstruction-based functionality into a novel self-supervised predictive architectural building block. The proposed self-supervised block is generic and can easily be incorporated into various state-of-the-art anomaly detection methods. Our block starts with a convolutional layer with dilated ﬁlters, where the center area of the receptive ﬁeld is masked. The resulting activation maps are passed through a channel attention module. Our block is equipped with a loss that minimizes the reconstruction error with respect to the masked area in the receptive ﬁeld. We demonstrate the generality of our block by integrating it into several state-of-the-art frameworks for anomaly detection on image and video, providing empirical evidence that shows considerable performance improvements on MVTec AD, Avenue, and ShanghaiTech. We release our code as open source at: https://github.com/ ristea/sspcab.</dcterms:abstract>
        <dc:date>6/2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9880272/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:46</dcterms:dateSubmitted>
        <bib:pages>13566-13576</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_16">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ristea 等 - 2022 - Self-Supervised Predictive Convolutional Attentive.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2106.08265">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Roth</foaf:surname>
                        <foaf:givenName>Karsten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pemula</foaf:surname>
                        <foaf:givenName>Latha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zepeda</foaf:surname>
                        <foaf:givenName>Joaquin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schölkopf</foaf:surname>
                        <foaf:givenName>Bernhard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brox</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gehler</foaf:surname>
                        <foaf:givenName>Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_177"/>
        <dcterms:isReferencedBy rdf:resource="#item_63"/>
        <dcterms:isReferencedBy rdf:resource="#item_180"/>
        <link:link rdf:resource="#item_17"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Towards Total Recall in Industrial Anomaly Detection</dc:title>
        <dcterms:abstract>Being able to spot defective parts is a critical component in large-scale industrial manufacturing. A particular challenge that we address in this work is the cold-start problem: ﬁt a model using nominal (non-defective) example images only. While handcrafted solutions per class are possible, the goal is to build systems that work well simultaneously on many different tasks automatically. The best peforming approaches combine embeddings from ImageNet models with an outlier detection model. In this paper, we extend on this line of work and propose PatchCore, which uses a maximally representative memory bank of nominal patchfeatures. PatchCore offers competitive inference times while achieving state-of-the-art performance for both detection and localization. On the challenging, widely used MVTec AD benchmark PatchCore achieves an image-level anomaly detection AUROC score of up to 99.6%, more than halving the error compared to the next best competitor. We further report competitive results on two additional datasets and also ﬁnd competitive results in the few samples regime. Code: github.com/amazon-research/patchcore-inspection.</dcterms:abstract>
        <dc:date>2022-05-05</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2106.08265</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:50</dcterms:dateSubmitted>
        <dc:description>arXiv:2106.08265 [cs]</dc:description>
        <prism:number>arXiv:2106.08265</prism:number>
    </rdf:Description>
   <bib:Memo rdf:about="#item_177"><rdf:value></rdf:value></bib:Memo>
    <bib:Memo rdf:about="#item_63">
       <rdf:value>Comment: Accepted to CVPR 2022</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_180">
        <rdf:value>&lt;div data-schema-version=&quot;8&quot;&gt;&lt;p&gt;Model: PatchCore&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_17">
        <z:itemType>attachment</z:itemType>
        <dc:title>Roth 等 - 2022 - Towards Total Recall in Industrial Anomaly Detecti.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2104.13897">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pirnay</foaf:surname>
                        <foaf:givenName>Jonathan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chai</foaf:surname>
                        <foaf:givenName>Keng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_65"/>
        <link:link rdf:resource="#item_18"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Inpainting Transformer for Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection in computer vision is the task of identifying images which deviate from a set of normal images. A common approach is to train deep convolutional autoencoders to inpaint covered parts of an image and compare the output with the original image. By training on anomaly-free samples only, the model is assumed to not being able to reconstruct anomalous regions properly. For anomaly detection by inpainting we suggest it to be beneﬁcial to incorporate information from potentially distant regions. In particular we pose anomaly detection as a patch-inpainting problem and propose to solve it with a purely self-attention based approach discarding convolutions. The proposed Inpainting Transformer (InTra) is trained to inpaint covered patches in a large sequence of image patches, thereby integrating information across large regions of the input image. When training from scratch, in comparison to other methods not using extra training data, InTra achieves results on par with the current state-of-the-art on the MVTec AD dataset for detection and surpassing them on segmentation.</dcterms:abstract>
        <dc:date>2021-11-26</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2104.13897</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:54</dcterms:dateSubmitted>
        <dc:description>arXiv:2104.13897 [cs]</dc:description>
        <prism:number>arXiv:2104.13897</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_65">
       <rdf:value>Comment: Accepted to ICIAP2021</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_18">
        <z:itemType>attachment</z:itemType>
        <dc:title>Pirnay 和 Chai - 2021 - Inpainting Transformer for Anomaly Detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-66540-915-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66540-915-5</dc:identifier>
                <dc:title>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</dc:title>
                <dc:identifier>DOI 10.1109/WACV51458.2022.00312</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Waikoloa, HI, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tsai</foaf:surname>
                        <foaf:givenName>Chin-Chia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Tsung-Hsuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lai</foaf:surname>
                        <foaf:givenName>Shang-Hong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_19"/>
        <dc:title>Multi-Scale Patch-Based Representation Learning for Image Anomaly Detection and Segmentation</dc:title>
        <dcterms:abstract>Unsupervised representation learning has been proven to be effective for the challenging anomaly detection and segmentation tasks. In this paper, we propose a multi-scale patch-based representation learning method to extract critical and representative information from normal images. By taking the relative feature similarity between patches of different local distances into account, we can achieve better representation learning. Moreover, we propose a refined way to improve the self-supervised learning strategy, thus allowing our model to learn better geometric relationship between neighboring patches. Through sliding patches of different scales all over an image, our model extracts representative features from each patch and compares them with those in the training set of normal images to detect the anomalous regions. Our experimental results on MVTec AD dataset and BTAD dataset demonstrate the proposed method achieves the state-of-the-art accuracy for both anomaly detection and segmentation.</dcterms:abstract>
        <dc:date>1/2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9707044/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:31:58</dcterms:dateSubmitted>
        <bib:pages>3065-3073</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_19">
        <z:itemType>attachment</z:itemType>
        <dc:title>Tsai 等 - 2022 - Multi-Scale Patch-Based Representation Learning fo.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9706997/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66540-915-5</dc:identifier>
                <dc:title>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</dc:title>
                <dc:identifier>DOI 10.1109/WACV51458.2022.00189</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Waikoloa, HI, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rudolph</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wehrbein</foaf:surname>
                        <foaf:givenName>Tom</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosenhahn</foaf:surname>
                        <foaf:givenName>Bodo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wandt</foaf:surname>
                        <foaf:givenName>Bastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_20"/>
        <dc:title>Fully Convolutional Cross-Scale-Flows for Image-based Defect Detection</dc:title>
        <dcterms:abstract>In industrial manufacturing processes, errors frequently occur at unpredictable times and in unknown manifestations. We tackle the problem of automatic defect detection without requiring any image samples of defective parts. Recent works model the distribution of defect-free image data, using either strong statistical priors or overly simpliﬁed data representations. In contrast, our approach handles ﬁne-grained representations incorporating the global and local image context while ﬂexibly estimating the density. To this end, we propose a novel fully convolutional cross-scale normalizing ﬂow (CS-Flow) that jointly processes multiple feature maps of different scales. Using normalizing ﬂows to assign meaningful likelihoods to input samples allows for efﬁcient defect detection on image-level. Moreover, due to the preserved spatial arrangement the latent space of the normalizing ﬂow is interpretable which enables to localize defective regions in the image. Our work sets a new stateof-the-art in image-level defect detection on the benchmark datasets Magnetic Tile Defects and MVTec AD showing a 100% AUROC on 4 out of 15 classes.</dcterms:abstract>
        <dc:date>1/2022</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9706997/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:02</dcterms:dateSubmitted>
        <bib:pages>1829-1838</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_20">
        <z:itemType>attachment</z:itemType>
        <dc:title>Rudolph 等 - 2022 - Fully Convolutional Cross-Scale-Flows for Image-ba.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2107.12571">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gudovskiy</foaf:surname>
                        <foaf:givenName>Denis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ishizaka</foaf:surname>
                        <foaf:givenName>Shun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kozuka</foaf:surname>
                        <foaf:givenName>Kazuki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_69"/>
        <link:link rdf:resource="#item_21"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CFLOW-AD: Real-Time Unsupervised Anomaly Detection with Localization via Conditional Normalizing Flows</dc:title>
        <dcterms:abstract>Unsupervised anomaly detection with localization has many practical applications when labeling is infeasible and, moreover, when anomaly examples are completely missing in the train data. While recently proposed models for such data setup achieve high accuracy metrics, their complexity is a limiting factor for real-time processing. In this paper, we propose a real-time model and analytically derive its relationship to prior methods. Our CFLOW-AD model is based on a conditional normalizing ﬂow framework adopted for anomaly detection with localization. In particular, CFLOW-AD consists of a discriminatively pretrained encoder followed by a multi-scale generative decoders where the latter explicitly estimate likelihood of the encoded features. Our approach results in a computationally and memory-efﬁcient model: CFLOW-AD is faster and smaller by a factor of 10× than prior state-of-the-art with the same input setting. Our experiments on the MVTec dataset show that CFLOW-AD outperforms previous methods by 0.36% AUROC in detection task, by 1.12% AUROC and 2.5% AUPRO in localization task, respectively. We open-source our code with fully reproducible experiments1.</dcterms:abstract>
        <dc:date>2021-07-26</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CFLOW-AD</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2107.12571</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:07</dcterms:dateSubmitted>
        <dc:description>arXiv:2107.12571 [cs]</dc:description>
        <prism:number>arXiv:2107.12571</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_69">
       <rdf:value>Comment: Accepted to WACV 2022. Preprint</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_21">
        <z:itemType>attachment</z:itemType>
        <dc:title>Gudovskiy 等 - 2021 - CFLOW-AD Real-Time Unsupervised Anomaly Detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:BookSection rdf:about="urn:isbn:978-3-030-69543-9%20978-3-030-69544-6">
        <z:itemType>bookSection</z:itemType>
        <dcterms:isPartOf>
            <bib:Book>
                <prism:volume>12627</prism:volume>
                <dc:identifier>ISBN 978-3-030-69543-9 978-3-030-69544-6</dc:identifier>
                <dc:title>Computer Vision – ACCV 2020</dc:title>
            </bib:Book>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Cham</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Springer International Publishing</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:editors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ishikawa</foaf:surname>
                        <foaf:givenName>Hiroshi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Cheng-Lin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pajdla</foaf:surname>
                        <foaf:givenName>Tomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shi</foaf:surname>
                        <foaf:givenName>Jianbo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:editors>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yi</foaf:surname>
                        <foaf:givenName>Jihun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoon</foaf:surname>
                        <foaf:givenName>Sungroh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_22"/>
        <dc:title>Patch SVDD: Patch-Level SVDD for Anomaly Detection and Segmentation</dc:title>
        <dcterms:abstract>In this paper, we address the problem of image anomaly detection and segmentation. Anomaly detection involves making a binary decision as to whether an input image contains an anomaly, and anomaly segmentation aims to locate the anomaly on the pixel level. Support vector data description (SVDD) is a long-standing algorithm used for an anomaly detection, and we extend its deep learning variant to the patch-based method using self-supervised learning. This extension enables anomaly segmentation and improves detection performance. As a result, anomaly detection and segmentation performances measured in AUROC on MVTec AD dataset increased by 9.8% and 7.0%, respectively, compared to the previous state-of-the-art methods. Our results indicate the eﬃcacy of the proposed method and its potential for industrial application. Detailed analysis of the proposed method oﬀers insights regarding its behavior, and the code is available online1.</dcterms:abstract>
        <dc:date>2021</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Patch SVDD</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1007/978-3-030-69544-6_23</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:10</dcterms:dateSubmitted>
        <dc:description>Series Title: Lecture Notes in Computer Science
DOI: 10.1007/978-3-030-69544-6_23</dc:description>
        <bib:pages>375-390</bib:pages>
    </bib:BookSection>
    <z:Attachment rdf:about="#item_22">
        <z:itemType>attachment</z:itemType>
        <dc:title>Yi 和 Yoon - 2021 - Patch SVDD Patch-Level SVDD for Anomaly Detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-72817-168-5">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.00424</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergmann</foaf:surname>
                        <foaf:givenName>Paul</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fauser</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sattlegger</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steger</foaf:surname>
                        <foaf:givenName>Carsten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_24"/>
        <dc:title>Uninformed Students: Student-Teacher Anomaly Detection With Discriminative Latent Embeddings</dc:title>
        <dcterms:abstract>We introduce a powerful student–teacher framework for the challenging problem of unsupervised anomaly detection and pixel-precise anomaly segmentation in high-resolution images. Student networks are trained to regress the output of a descriptive teacher network that was pretrained on a large dataset of patches from natural images. This circumvents the need for prior data annotation. Anomalies are detected when the outputs of the student networks differ from that of the teacher network. This happens when they fail to generalize outside the manifold of anomaly-free training data. The intrinsic uncertainty in the student networks is used as an additional scoring function that indicates anomalies. We compare our method to a large number of existing deep learning based methods for unsupervised anomaly detection. Our experiments demonstrate improvements over state-of-the-art methods on a number of realworld datasets, including the recently introduced MVTec Anomaly Detection dataset that was speciﬁcally designed to benchmark anomaly segmentation algorithms.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Uninformed Students</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9157778/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:19</dcterms:dateSubmitted>
        <bib:pages>4182-4191</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_24">
        <z:itemType>attachment</z:itemType>
        <dc:title>Bergmann 等 - 2020 - Uninformed Students Student-Teacher Anomaly Detec.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_75">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yamada</foaf:surname>
                        <foaf:givenName>Shinji</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hotta</foaf:surname>
                        <foaf:givenName>Kazuhiro</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_26"/>
        <dc:title>Reconstruction Student with Attention for Student-Teacher Pyramid Matching</dc:title>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_26">
        <z:itemType>attachment</z:itemType>
        <dc:title>Yamada 和 Hotta - Reconstruction Student with Attention for Student-.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2105.14737">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Jin-Hwa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kim</foaf:surname>
                        <foaf:givenName>Do-Hyeong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yi</foaf:surname>
                        <foaf:givenName>Saehoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Taehoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_27"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Semi-orthogonal Embedding for Efficient Unsupervised Anomaly Segmentation</dc:title>
        <dcterms:abstract>We present the efﬁciency of semi-orthogonal embedding for unsupervised anomaly segmentation. The multi-scale features from pre-trained CNNs are recently used for the localized Mahalanobis distances with signiﬁcant performance. However, the increased feature size is problematic to scale up to the bigger CNNs, since it requires the batch-inverse of multi-dimensional covariance tensor. Here, we generalize an ad-hoc method, random feature selection, into semi-orthogonal embedding for robust approximation, cubically reducing the computational cost for the inverse of multi-dimensional covariance tensor. With the scrutiny of ablation studies, the proposed method achieves a new state-of-the-art with signiﬁcant margins for the MVTec AD, KolektorSDD, KolektorSDD2, and mSTC datasets. The theoretical and empirical analyses offer insights and veriﬁcation of our straightforward yet cost-effective approach.</dcterms:abstract>
        <dc:date>2021-05-31</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2105.14737</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:29</dcterms:dateSubmitted>
        <dc:description>arXiv:2105.14737 [cs]</dc:description>
        <prism:number>arXiv:2105.14737</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_27">
        <z:itemType>attachment</z:itemType>
        <dc:title>Kim 等 - 2021 - Semi-orthogonal Embedding for Efficient Unsupervis.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2111.07677">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yu</foaf:surname>
                        <foaf:givenName>Jiawei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Ye</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Xiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Yushuang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhao</foaf:surname>
                        <foaf:givenName>Rui</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Liwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_78"/>
        <link:link rdf:resource="#item_28"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>FastFlow: Unsupervised Anomaly Detection and Localization via 2D Normalizing Flows</dc:title>
        <dcterms:abstract>Unsupervised anomaly detection and localization is crucial to the practical application when collecting and labeling sufﬁcient anomaly data is infeasible. Most existing representation-based approaches extract normal image features with a deep convolutional neural network and characterize the corresponding distribution through non-parametric distribution estimation methods. The anomaly score is calculated by measuring the distance between the feature of the test image and the estimated distribution. However, current methods can not effectively map image features to a tractable base distribution and ignore the relationship between local and global features which are important to identify anomalies. To this end, we propose FastFlow implemented with 2D normalizing ﬂows and use it as the probability distribution estimator. Our FastFlow can be used as a plug-in module with arbitrary deep feature extractors such as ResNet and vision transformer for unsupervised anomaly detection and localization. In training phase, FastFlow learns to transform the input visual feature into a tractable distribution and obtains the likelihood to recognize anomalies in inference phase. Extensive experimental results on the MVTec AD dataset show that FastFlow surpasses previous state-of-the-art methods in terms of accuracy and inference efﬁciency with various backbone networks. Our approach achieves 99.4% AUC in anomaly detection with high inference efﬁciency.</dcterms:abstract>
        <dc:date>2021-11-16</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>FastFlow</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2111.07677</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:34</dcterms:dateSubmitted>
        <dc:description>arXiv:2111.07677 [cs]</dc:description>
        <prism:number>arXiv:2111.07677</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_78">
       <rdf:value>Comment: 11 pages,8 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_28">
        <z:itemType>attachment</z:itemType>
        <dc:title>Yu 等 - 2021 - FastFlow Unsupervised Anomaly Detection and Local.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1805.10917">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Golan</foaf:surname>
                        <foaf:givenName>Izhak</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>El-Yaniv</foaf:surname>
                        <foaf:givenName>Ran</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_38"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Anomaly Detection Using Geometric Transformations</dc:title>
        <dcterms:abstract>We consider the problem of anomaly detection in images, and present a new detection technique. Given a sample of images, all known to belong to a “normal” class (e.g., dogs), we show how to train a deep neural model that can detect out-of-distribution images (i.e., non-dog objects). The main idea behind our scheme is to train a multi-class model to discriminate between dozens of geometric transformations applied on all the given images. The auxiliary expertise learned by the model generates feature detectors that effectively identify, at test time, anomalous images based on the softmax activation statistics of the model when applied on transformed images. We present extensive experiments using the proposed detector, which indicate that our technique consistently improves all known algorithms by a wide margin.</dcterms:abstract>
        <dc:date>2018-11-09</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1805.10917</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:40</dcterms:dateSubmitted>
        <dc:description>arXiv:1805.10917 [cs, stat]</dc:description>
        <prism:number>arXiv:1805.10917</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_38">
        <z:itemType>attachment</z:itemType>
        <dc:title>Golan 和 El-Yaniv - 2018 - Deep Anomaly Detection Using Geometric Transformat.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1904.02639">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gong</foaf:surname>
                        <foaf:givenName>Dong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Lingqiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Le</foaf:surname>
                        <foaf:givenName>Vuong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Saha</foaf:surname>
                        <foaf:givenName>Budhaditya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mansour</foaf:surname>
                        <foaf:givenName>Moussa Reda</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Venkatesh</foaf:surname>
                        <foaf:givenName>Svetha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hengel</foaf:surname>
                        <foaf:givenName>Anton van den</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_82"/>
        <link:link rdf:resource="#item_39"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</dc:title>
        <dcterms:abstract>Deep autoencoder has been extensively used for anomaly detection. Training on the normal data, the autoencoder is expected to produce higher reconstruction error for the abnormal inputs than the normal ones, which is adopted as a criterion for identifying anomalies. However, this assumption does not always hold in practice. It has been observed that sometimes the autoencoder “generalizes” so well that it can also reconstruct anomalies well, leading to the miss detection of anomalies. To mitigate this drawback for autoencoder based anomaly detector, we propose to augment the autoencoder with a memory module and develop an improved autoencoder called memory-augmented autoencoder, i.e. MemAE. Given an input, MemAE ﬁrstly obtains the encoding from the encoder and then uses it as a query to retrieve the most relevant memory items for reconstruction. At the training stage, the memory contents are updated and are encouraged to represent the prototypical elements of the normal data. At the test stage, the learned memory will be ﬁxed, and the reconstruction is obtained from a few selected memory records of the normal data. The reconstruction will thus tend to be close to a normal sample. Thus the reconstructed errors on anomalies will be strengthened for anomaly detection. MemAE is free of assumptions on the data type and thus general to be applied to different tasks. Experiments on various datasets prove the excellent generalization and high effectiveness of the proposed MemAE.</dcterms:abstract>
        <dc:date>2019-08-06</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Memorizing Normality to Detect Anomaly</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1904.02639</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:44</dcterms:dateSubmitted>
        <dc:description>arXiv:1904.02639 [cs]</dc:description>
        <prism:number>arXiv:1904.02639</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_82">
       <rdf:value>Comment: Accepted to appear at ICCV 2019</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_39">
        <z:itemType>attachment</z:itemType>
        <dc:title>Gong 等 - 2019 - Memorizing Normality to Detect Anomaly Memory-aug.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1812.04606">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hendrycks</foaf:surname>
                        <foaf:givenName>Dan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mazeika</foaf:surname>
                        <foaf:givenName>Mantas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dietterich</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_84"/>
        <link:link rdf:resource="#item_40"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Computation and Language</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Anomaly Detection with Outlier Exposure</dc:title>
        <dcterms:abstract>It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magniﬁes the difﬁculty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small- and large-scale vision tasks, we ﬁnd that Outlier Exposure signiﬁcantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the ﬂexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.</dcterms:abstract>
        <dc:date>2019-01-28</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1812.04606</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:48</dcterms:dateSubmitted>
        <dc:description>arXiv:1812.04606 [cs, stat]</dc:description>
        <prism:number>arXiv:1812.04606</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_84">
        <rdf:value>Comment: ICLR 2019; PyTorch code available at https://github.com/hendrycks/outlier-exposure</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_40">
        <z:itemType>attachment</z:itemType>
        <dc:title>Hendrycks 等 - 2019 - Deep Anomaly Detection with Outlier Exposure.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1901.08954">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akçay</foaf:surname>
                        <foaf:givenName>Samet</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Atapour-Abarghouei</foaf:surname>
                        <foaf:givenName>Amir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Breckon</foaf:surname>
                        <foaf:givenName>Toby P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_86"/>
        <link:link rdf:resource="#item_41"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Skip-GANomaly: Skip Connected and Adversarially Trained Encoder-Decoder Anomaly Detection</dc:title>
        <dcterms:abstract>Despite inherent ill-deﬁnition, anomaly detection is a research endeavour of great interest within machine learning and visual scene understanding alike. Most commonly, anomaly detection is considered as the detection of outliers within a given data distribution based on some measure of normality. The most signiﬁcant challenge in real-world anomaly detection problems is that available data is highly imbalanced towards normality (i.e. non-anomalous) and contains a most a sub-set of all possible anomalous samples - hence limiting the use of well-established supervised learning methods. By contrast, we introduce an unsupervised anomaly detection model, trained only on the normal (non-anomalous, plentiful) samples in order to learn the normality distribution of the domain, and hence detect abnormality based on deviation from this model. Our proposed approach employs an encoder-decoder convolutional neural network with skip connections to thoroughly capture the multi-scale distribution of the normal data distribution in highdimensional image space. Furthermore, utilizing an adversarial training scheme for this chosen architecture provides superior reconstruction both within high-dimensional image space and a lower-dimensional latent vector space encoding. Minimizing the reconstruction error metric within both the image and hidden vector spaces during training aids the model to learn the distribution of normality as required. Higher reconstruction metrics during subsequent test and deployment are thus indicative of a deviation from this normal distribution, hence indicative of an anomaly. Experimentation over established anomaly detection benchmarks and challenging real-world datasets, within the context of X-ray security screening, shows the unique promise of such a proposed approach.</dcterms:abstract>
        <dc:date>2019-01-25</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Skip-GANomaly</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1901.08954</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:52</dcterms:dateSubmitted>
        <dc:description>arXiv:1901.08954 [cs]</dc:description>
        <prism:number>arXiv:1901.08954</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_86">
       <rdf:value>Comment: Conference Submission. 8 pages, 9 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_41">
        <z:itemType>attachment</z:itemType>
        <dc:title>Akçay 等 - 2019 - Skip-GANomaly Skip Connected and Adversarially Tr.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9157105/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-72817-168-5</dc:identifier>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR42600.2020.01419</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Seattle, WA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zaigham Zaheer</foaf:surname>
                        <foaf:givenName>Muhammad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Jin-Ha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Astrid</foaf:surname>
                        <foaf:givenName>Marcella</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lee</foaf:surname>
                        <foaf:givenName>Seung-Ik</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_42"/>
        <dc:title>Old Is Gold: Redefining the Adversarially Learned One-Class Classifier Training Paradigm</dc:title>
        <dcterms:abstract>A popular method for anomaly detection is to use the generator of an adversarial network to formulate anomaly score over reconstruction loss of input. Due to the rare occurrence of anomalies, optimizing such networks can be a cumbersome task. Another possible approach is to use both generator and discriminator for anomaly detection. However, attributed to the involvement of adversarial training, this model is often unstable in a way that the performance ﬂuctuates drastically with each training step. In this study, we propose a framework that effectively generates stable results across a wide range of training steps and allows us to use both the generator and the discriminator of an adversarial model for efﬁcient and robust anomaly detection. Our approach transforms the fundamental role of a discriminator from identifying real and fake data to distinguishing between good and bad quality reconstructions. To this end, we prepare training examples for the good quality reconstruction by employing the current generator, whereas poor quality examples are obtained by utilizing an old state of the same generator. This way, the discriminator learns to detect subtle distortions that often appear in reconstructions of the anomaly inputs. Extensive experiments performed on Caltech-256 and MNIST image datasets for novelty detection show superior results. Furthermore, on UCSD Ped2 video dataset for anomaly detection, our model achieves a frame-level AUC of 98.1%, surpassing recent state-of-theart methods.</dcterms:abstract>
        <dc:date>6/2020</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Old Is Gold</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9157105/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:32:56</dcterms:dateSubmitted>
        <bib:pages>14171-14181</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_42">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zaigham Zaheer 等 - 2020 - Old Is Gold Redefining the Adversarially Learned .pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2005.02359">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bergman</foaf:surname>
                        <foaf:givenName>Liron</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoshen</foaf:surname>
                        <foaf:givenName>Yedid</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_89"/>
        <link:link rdf:resource="#item_43"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Classification-Based Anomaly Detection for General Data</dc:title>
        <dcterms:abstract>Anomaly detection, ﬁnding patterns that substantially deviate from those seen previously, is one of the fundamental problems of artiﬁcial intelligence. Recently, classiﬁcation-based methods were shown to achieve superior results on this task. In this work, we present a unifying view and propose an open-set method, GOAD, to relax current generalization assumptions. Furthermore, we extend the applicability of transformation-based methods to non-image data using random afﬁne transformations. Our method is shown to obtain state-of-the-art accuracy and is applicable to broad data types. The strong performance of our method is extensively validated on multiple datasets from different domains.</dcterms:abstract>
        <dc:date>2020-05-05</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.02359</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:33:00</dcterms:dateSubmitted>
        <dc:description>arXiv:2005.02359 [cs, stat]</dc:description>
        <prism:number>arXiv:2005.02359</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_89">
       <rdf:value>Comment: ICLR'20</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_43">
        <z:itemType>attachment</z:itemType>
        <dc:title>Bergman 和 Hoshen - 2020 - Classification-Based Anomaly Detection for General.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_90">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ruff</foaf:surname>
                        <foaf:givenName>Lukas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vandermeulen</foaf:surname>
                        <foaf:givenName>Robert A</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Görnitz</foaf:surname>
                        <foaf:givenName>Nico</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Binder</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Müller</foaf:surname>
                        <foaf:givenName>Emmanuel</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Müller</foaf:surname>
                        <foaf:givenName>Klaus-Robert</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kloft</foaf:surname>
                        <foaf:givenName>Marius</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_46"/>
        <dc:title>DEEP SEMI-SUPERVISED ANOMALY DETECTION</dc:title>
        <dcterms:abstract>Deep approaches to anomaly detection have recently shown promising results over shallow methods on large and complex datasets. Typically anomaly detection is treated as an unsupervised learning problem. In practice however, one may have—in addition to a large set of unlabeled samples—access to a small pool of labeled samples, e.g. a subset veriﬁed by some domain expert as being normal or anomalous. Semi-supervised approaches to anomaly detection aim to utilize such labeled samples, but most proposed methods are limited to merely including labeled normal samples. Only a few methods take advantage of labeled anomalies, with existing deep approaches being domain-speciﬁc. In this work we present Deep SAD, an end-to-end deep methodology for general semi-supervised anomaly detection. We further introduce an information-theoretic framework for deep anomaly detection based on the idea that the entropy of the latent distribution for normal data should be lower than the entropy of the anomalous distribution, which can serve as a theoretical interpretation for our method. In extensive experiments on MNIST, Fashion-MNIST, and CIFAR-10, along with other anomaly detection benchmark datasets, we demonstrate that our method is on par or outperforms shallow, hybrid, and deep competitors, yielding appreciable performance improvements even when provided with only little labeled data.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_46">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ruff 等 - 2020 - DEEP SEMI-SUPERVISED ANOMALY DETECTION.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2007.08176">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tack</foaf:surname>
                        <foaf:givenName>Jihoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mo</foaf:surname>
                        <foaf:givenName>Sangwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jeong</foaf:surname>
                        <foaf:givenName>Jongheon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shin</foaf:surname>
                        <foaf:givenName>Jinwoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_92"/>
        <link:link rdf:resource="#item_47"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances</dc:title>
        <dcterms:abstract>Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Speciﬁcally, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is speciﬁc to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets. Code and pre-trained models are available at https://github.com/alinlab/CSI.</dcterms:abstract>
        <dc:date>2020-10-21</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>CSI</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2007.08176</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:33:06</dcterms:dateSubmitted>
        <dc:description>arXiv:2007.08176 [cs, stat]</dc:description>
        <prism:number>arXiv:2007.08176</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_92">
        <rdf:value>Comment: NeurIPS 2020. First two authors contributed equally</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_47">
        <z:itemType>attachment</z:itemType>
        <dc:title>Tack 等 - 2020 - CSI Novelty Detection via Contrastive Learning on.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1805.06725">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Akcay</foaf:surname>
                        <foaf:givenName>Samet</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Atapour-Abarghouei</foaf:surname>
                        <foaf:givenName>Amir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Breckon</foaf:surname>
                        <foaf:givenName>Toby P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_94"/>
        <link:link rdf:resource="#item_48"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>GANomaly: Semi-Supervised Anomaly Detection via Adversarial Training</dc:title>
        <dcterms:abstract>Anomaly detection is a classical problem in computer vision, namely the determination of the normal from the abnormal when datasets are highly biased towards one class (normal) due to the insuﬃcient sample size of the other class (abnormal). While this can be addressed as a supervised learning problem, a signiﬁcantly more challenging problem is that of detecting the unknown/unseen anomaly case that takes us instead into the space of a one-class, semi-supervised learning paradigm. We introduce such a novel anomaly detection model, by using a conditional generative adversarial network that jointly learns the generation of high-dimensional image space and the inference of latent space. Employing encoder-decoder-encoder sub-networks in the generator network enables the model to map the input image to a lower dimension vector, which is then used to reconstruct the generated output image. The use of the additional encoder network maps this generated image to its latent representation. Minimizing the distance between these images and the latent vectors during training aids in learning the data distribution for the normal samples. As a result, a larger distance metric from this learned data distribution at inference time is indicative of an outlier from that distribution — an anomaly. Experimentation over several benchmark datasets, from varying domains, shows the model eﬃcacy and superiority over previous state-of-the-art approaches.</dcterms:abstract>
        <dc:date>2018-11-13</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>GANomaly</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1805.06725</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:33:10</dcterms:dateSubmitted>
        <dc:description>arXiv:1805.06725 [cs]</dc:description>
        <prism:number>arXiv:1805.06725</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_94">
       <rdf:value>Comment: ACCV 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_48">
        <z:itemType>attachment</z:itemType>
        <dc:title>Akcay 等 - 2018 - GANomaly Semi-Supervised Anomaly Detection via Ad.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1812.02288">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zenati</foaf:surname>
                        <foaf:givenName>Houssam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Romain</foaf:surname>
                        <foaf:givenName>Manon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Foo</foaf:surname>
                        <foaf:givenName>Chuan Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lecouat</foaf:surname>
                        <foaf:givenName>Bruno</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandrasekhar</foaf:surname>
                        <foaf:givenName>Vijay Ramaseshan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_96"/>
        <link:link rdf:resource="#item_49"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Adversarially Learned Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection is a signiﬁcant and hence wellstudied problem. However, developing effective anomaly detection methods for complex and high-dimensional data remains a challenge. As Generative Adversarial Networks (GANs) are able to model the complex high-dimensional distributions of real-world data, they offer a promising approach to address this challenge. In this work, we propose an anomaly detection method, Adversarially Learned Anomaly Detection (ALAD) based on bi-directional GANs, that derives adversarially learned features for the anomaly detection task. ALAD then uses reconstruction errors based on these adversarially learned features to determine if a data sample is anomalous. ALAD builds on recent advances to ensure data-space and latent-space cycle-consistencies and stabilize GAN training, which results in signiﬁcantly improved anomaly detection performance. ALAD achieves state-of-the-art performance on a range of image and tabular datasets while being several hundred-fold faster at test time than the only published GAN-based method.</dcterms:abstract>
        <dc:date>2018-12-05</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1812.02288</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:33:14</dcterms:dateSubmitted>
        <dc:description>arXiv:1812.02288 [cs, stat]</dc:description>
        <prism:number>arXiv:1812.02288</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_96">
        <rdf:value>Comment: In the Proceedings of the 20th IEEE International Conference on Data Mining (ICDM), 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_49">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zenati 等 - 2018 - Adversarially Learned Anomaly Detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1802.06222">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zenati</foaf:surname>
                        <foaf:givenName>Houssam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Foo</foaf:surname>
                        <foaf:givenName>Chuan Sheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lecouat</foaf:surname>
                        <foaf:givenName>Bruno</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Manek</foaf:surname>
                        <foaf:givenName>Gaurav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandrasekhar</foaf:surname>
                        <foaf:givenName>Vijay Ramaseshan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_98"/>
        <link:link rdf:resource="#item_50"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Efficient GAN-Based Anomaly Detection</dc:title>
        <dcterms:abstract>Generative adversarial networks (GANs) are able to model the complex highdimensional distributions of real-world data, which suggests they could be effective for anomaly detection. However, few works have explored the use of GANs for the anomaly detection task. We leverage recently developed GAN models for anomaly detection, and achieve state-of-the-art performance on image and network intrusion datasets, while being several hundred-fold faster at test time than the only published GAN-based method.</dcterms:abstract>
        <dc:date>2019-05-01</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1802.06222</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:33:18</dcterms:dateSubmitted>
        <dc:description>arXiv:1802.06222 [cs, stat]</dc:description>
        <prism:number>arXiv:1802.06222</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_98">
        <rdf:value>Comment: Updated version of this work is published at ICDM 2018, see arXiv:1812.02288 . Submitted to the ICLR Workshop 2018</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_50">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zenati 等 - 2019 - Efficient GAN-Based Anomaly Detection.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_113">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zong</foaf:surname>
                        <foaf:givenName>Bo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Song</foaf:surname>
                        <foaf:givenName>Qi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Min</foaf:surname>
                        <foaf:givenName>Martin Renqiang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cheng</foaf:surname>
                        <foaf:givenName>Wei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lumezanu</foaf:surname>
                        <foaf:givenName>Cristian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>Daeki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Haifeng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_112"/>
        <dc:title>DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL FOR UNSUPERVISED ANOMALY DETECTION</dc:title>
        <dcterms:abstract>Unsupervised anomaly detection on multi- or high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM signiﬁcantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_112">
        <z:itemType>attachment</z:itemType>
        <dc:title>Zong 等 - 2018 - DEEP AUTOENCODING GAUSSIAN MIXTURE MODEL FOR UNSUP.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2301.11514">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jiaqi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Guoyang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jingbao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Shangnian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chengjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jin</foaf:surname>
                        <foaf:givenName>Yaochu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_114"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Deep Industrial Image Anomaly Detection: A Survey</dc:title>
        <dcterms:abstract>The recent rapid development of deep learning has laid a milestone in industrial Image Anomaly Detection (IAD). In this paper, we provide a comprehensive review of deep learning-based image anomaly detection techniques, from the perspectives of neural network architectures, levels of supervision, loss functions, metrics and datasets. In addition, we extract the new setting from industrial manufacturing and review the current IAD approaches under our proposed our new setting. Moreover, we highlight several opening challenges for image anomaly detection. The merits and downsides of representative network architectures under varying supervision are discussed. Finally, we summarize the research ﬁndings and point out future research directions. More resources are available at https://github.com/M-3LAB/awesome-industrial-anomaly-detection.</dcterms:abstract>
        <dc:date>2023-05-29</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Deep Industrial Image Anomaly Detection</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2301.11514</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:38:58</dcterms:dateSubmitted>
        <dc:description>arXiv:2301.11514 [cs]</dc:description>
        <prism:number>arXiv:2301.11514</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_114">
        <z:itemType>attachment</z:itemType>
        <dc:title>Liu 等 - 2023 - Deep Industrial Image Anomaly Detection A Survey.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://ieeexplore.ieee.org/document/9879727/">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66546-946-3</dc:identifier>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
                <dc:identifier>DOI 10.1109/CVPR52688.2022.00724</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New Orleans, LA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ding</foaf:surname>
                        <foaf:givenName>Choubo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pang</foaf:surname>
                        <foaf:givenName>Guansong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shen</foaf:surname>
                        <foaf:givenName>Chunhua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_116"/>
        <dc:title>Catching Both Gray and Black Swans: Open-set Supervised Anomaly Detection</dc:title>
        <dc:date>6/2022</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Catching Both Gray and Black Swans</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9879727/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:40:17</dcterms:dateSubmitted>
        <bib:pages>7378-7388</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_116">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ding 等 - 2022 - Catching Both Gray and Black Swans Open-set Super.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_150">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ester</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kriegel</foaf:surname>
                        <foaf:givenName>Hans-Peter</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sander</foaf:surname>
                        <foaf:givenName>Jörg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Xiaowei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_149"/>
        <dc:title>A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise</dc:title>
        <dcterms:abstract>Clustering algorithms are attractive for the task of class identiﬁcation in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efﬁciency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm DBSCAN relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. DBSCAN requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efﬁciency of DBSCAN using synthetic data and real data of the SEQUOIA 2000 benchmark. The results of our experiments demonstrate that (1) DBSCAN is signiﬁcantly more effective in discovering clusters of arbitrary shape than the well-known algorithm CLARANS, and that (2) DBSCAN outperforms CLARANS by a factor of more than 100 in terms of efﬁciency.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_149">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ester 等 - A Density-Based Algorithm for Discovering Clusters.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://ieeexplore.ieee.org/document/1284395/">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:1057-7149"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Z.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bovik</foaf:surname>
                        <foaf:givenName>A.C.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sheikh</foaf:surname>
                        <foaf:givenName>H.R.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Simoncelli</foaf:surname>
                        <foaf:givenName>E.P.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_151"/>
        <dc:title>Image Quality Assessment: From Error Visibility to Structural Similarity</dc:title>
        <dc:date>04/2004</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Image Quality Assessment</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/1284395/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:42:37</dcterms:dateSubmitted>
        <bib:pages>600-612</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:1057-7149">
        <prism:volume>13</prism:volume>
        <dc:title>IEEE Transactions on Image Processing</dc:title>
        <dc:identifier>DOI 10.1109/TIP.2003.819861</dc:identifier>
        <prism:number>4</prism:number>
        <dcterms:alternative>IEEE Trans. on Image Process.</dcterms:alternative>
        <dc:identifier>ISSN 1057-7149</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_151">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wang 等 - 2004 - Image Quality Assessment From Error Visibility to.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://scienceopen.com/hosted-document?doi=10.14236/ewic/VOCS2008.18">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
               <dc:identifier>DOI 10.14236/ewic/VOCS2008.18</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pokrajac</foaf:surname>
                        <foaf:givenName>Dragoljub</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reljin</foaf:surname>
                        <foaf:givenName>Natasa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pejcic</foaf:surname>
                        <foaf:givenName>Nebojsa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lazarevic</foaf:surname>
                        <foaf:givenName>Aleksandar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_153"/>
        <dc:title>Incremental Connectivity-Based Outlier Factor Algorithm</dc:title>
        <dcterms:abstract>Outlier detection has recently become an important problem in many industrial and financial applications. Often, outliers have to be detected from data streams that continuously arrive from data sources. Incremental outlier detection algorithms, aimed at detecting outliers as soon as they appear in a database, have recently become emerging research field. In this paper, we develop an incremental version of connectivity-based outlier factor (COF) algorithm and discuss its computational complexity. The proposed incremental COF algorithm has equivalent detection performance as the iterated static COF algorithm (applied after insertion of each data record), with significant reduction in computational time. The paper provides theoretical and experimental evidence that the number of updates per such insertion/deletion does not depend on the total number of points in the data set, which makes algorithm viable for very large dynamic datasets. Finally, we also illustrate an application of the proposed algorithm on motion detection in video surveillance applications.</dcterms:abstract>
        <dc:date>09/2008</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://scienceopen.com/hosted-document?doi=10.14236/ewic/VOCS2008.18</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:43:29</dcterms:dateSubmitted>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>Visions of Computer Science - BCS International Academic Conference</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_153">
        <z:itemType>attachment</z:itemType>
        <dc:title>Pokrajac 等 - 2008 - Incremental Connectivity-Based Outlier Factor Algo.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-0-7695-3502-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-0-7695-3502-9</dc:identifier>
                <dc:title>2008 Eighth IEEE International Conference on Data Mining</dc:title>
                <dc:identifier>DOI 10.1109/ICDM.2008.17</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Pisa, Italy</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Fei Tony</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ting</foaf:surname>
                        <foaf:givenName>Kai Ming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Zhi-Hua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_155"/>
        <dc:title>Isolation Forest</dc:title>
        <dcterms:abstract>Most existing model-based approaches to anomaly detection construct a proﬁle of normal instances, then identify instances that do not conform to the normal proﬁle as anomalies. This paper proposes a fundamentally different model-based method that explicitly isolates anomalies instead of proﬁles normal points. To our best knowledge, the concept of isolation has not been explored in current literature. The use of isolation enables the proposed method, iForest, to exploit sub-sampling to an extent that is not feasible in existing methods, creating an algorithm which has a linear time complexity with a low constant and a low memory requirement. Our empirical evaluation shows that iForest performs favourably to ORCA, a near-linear time complexity distance-based method, LOF and Random Forests in terms of AUC and processing time, and especially in large data sets. iForest also works well in high dimensional problems which have a large number of irrelevant attributes, and in situations where training set does not contain any anomalies.</dcterms:abstract>
        <dc:date>12/2008</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://ieeexplore.ieee.org/document/4781136/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:43:44</dcterms:dateSubmitted>
        <bib:pages>413-422</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2008 Eighth IEEE International Conference on Data Mining (ICDM)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_155">
        <z:itemType>attachment</z:itemType>
        <dc:title>Liu 等 - 2008 - Isolation Forest.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_158">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jiang</foaf:surname>
                        <foaf:givenName>Xi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Jianlin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Jinbao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nie</foaf:surname>
                        <foaf:givenName>Qian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wu</foaf:surname>
                        <foaf:givenName>Kai</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Yong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Chengjie</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zheng</foaf:surname>
                        <foaf:givenName>Feng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_157"/>
        <dc:title>SoftPatch: Unsupervised Anomaly Detection with Noisy Data</dc:title>
        <dcterms:abstract>Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in various noise scenes demonstrate that SoftPatch outperforms the state-of-the-art AD methods on the MVTecAD and BTAD benchmarks and is comparable to those methods under the setting without noise.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_157">
        <z:itemType>attachment</z:itemType>
        <dc:title>Jiang 等 - SoftPatch Unsupervised Anomaly Detection with Noi.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://linkinghub.elsevier.com/retrieve/pii/S1877050920320524">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:18770509"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nowak - Brzezińska</foaf:surname>
                        <foaf:givenName>Agnieszka</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Horyń</foaf:surname>
                        <foaf:givenName>Czesław</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_159"/>
        <dc:title>Outliers in rules - the comparision of LOF, COF and KMEANS algorithms.</dc:title>
        <dcterms:abstract>Abstract The aim of the article is the analysis of using LOF, COF and Kmeans algorithms for outlier detection in rule based knowledge bTahseesa.imThoefstuhbejeacrttiocfleouistlitehremaninailnygsiissovferuysiinmgpLoOrtaFn,tCnoOwFadaanyds.KOmuetlainerssailngrourliethsmmseafonruonuutsliuearl druetleecstwiohnicihn arurelerabraesiendckonmopwalreidsogne tboasoetsh.eTrshaensdubshjeocutlodfboeuetlxieprlomreindinfugrtihsevrebryy itmhepodrotmanatinnoewxpaderaty. sI.nOthuetlireersseainrcrhultehsemauetahnoursnuussueatlhreuoleustlwiehridcehteacretioranremienthcoomdsptaoriﬁsonnd atogoivtheenrs(1a%nd, 5sh%o,u1ld0%be)enxupmlobreerdoffurotuhtelriebrys itnherudloems.aTinheenx,ptehret.yInanthaleyrzeesewahrcichhthoef aseuvtheonrvsaursioeuthsequoaultiltiyerinddetieccetsi,otnhamt eththeoydussteodﬁfnodr aallgirvuelens(1an%d, a5f%te,r 1re0m%o)vniunmg bseelreoctfeoduotluietlriserisn, riumlepsr.ovTehethne, tqhueayliatynaolyf zreulwe hcilcuhstoerfss.eIvnenthveaerxiopuesriqmueanlittayl isntadgiceetsh,ethaautththoerys uusseedd sfoixr dalilﬀreurleenst aknndowaflteedrgreembaosveisn.gThseelerecsteudltsoushtloiewrst,hiamt pthreovoepttihmeaqluraelsiutyltsofwreurleeacclhuisetveersd. fIonrtCheOeFxpoeurtilmieerndteatlescttaiogne athlgeoaruitthhmorassutsheedosnixe fdoiﬀr ewrhenicthk,naomwolnedggaellbaansaelsy.zTehdeqrueasluitlytsisnhdoiwcesth, athtethceluosptetirmqaularleitsyulitms pwreorveedacmhioesvtefdrefqoureCnOtlyF. outlier detection algorithm as the one for which, among all analyzed quality indices, the cluster quality improved most frequently.</dcterms:abstract>
        <dc:date>2020</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://linkinghub.elsevier.com/retrieve/pii/S1877050920320524</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:44:44</dcterms:dateSubmitted>
        <bib:pages>1420-1429</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:18770509">
        <prism:volume>176</prism:volume>
        <dc:title>Procedia Computer Science</dc:title>
        <dc:identifier>DOI 10.1016/j.procs.2020.09.152</dc:identifier>
        <dcterms:alternative>Procedia Computer Science</dcterms:alternative>
        <dc:identifier>ISSN 18770509</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_159">
        <z:itemType>attachment</z:itemType>
        <dc:title>Nowak - Brzezińska 和 Horyń - 2020 - Outliers in rules - the comparision of LOF, COF an.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1512.03385">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xiangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Shaoqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_163"/>
        <dcterms:isReferencedBy rdf:resource="#item_197"/>
        <link:link rdf:resource="#item_161"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>resnet</dc:subject>
        <dc:title>Deep Residual Learning for Image Recognition</dc:title>
        <dcterms:abstract>Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.</dcterms:abstract>
        <dc:date>2015-12-10</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1512.03385</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:45:10</dcterms:dateSubmitted>
        <dc:description>arXiv:1512.03385 [cs]</dc:description>
        <prism:number>arXiv:1512.03385</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_163">
       <rdf:value>Comment: Tech report</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_197">
        <rdf:value>&lt;div data-schema-version=&quot;8&quot;&gt;&lt;p&gt;Resnet&lt;/p&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_161">
        <z:itemType>attachment</z:itemType>
        <dc:title>He 等 - 2015 - Deep Residual Learning for Image Recognition.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1603.05027">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xiangyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ren</foaf:surname>
                        <foaf:givenName>Shaoqing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sun</foaf:surname>
                        <foaf:givenName>Jian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_166"/>
        <link:link rdf:resource="#item_164"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Identity Mappings in Deep Residual Networks</dc:title>
        <dcterms:abstract>Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.</dcterms:abstract>
        <dc:date>2016-07-25</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1603.05027</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:45:16</dcterms:dateSubmitted>
        <dc:description>arXiv:1603.05027 [cs]</dc:description>
        <prism:number>arXiv:1603.05027</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_166">
       <rdf:value>Comment: ECCV 2016 camera-ready</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_164">
        <z:itemType>attachment</z:itemType>
        <dc:title>He 等 - 2016 - Identity Mappings in Deep Residual Networks.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2303.15140">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Liu</foaf:surname>
                        <foaf:givenName>Zhikang</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Yiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Yuansheng</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wang</foaf:surname>
                        <foaf:givenName>Zilei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_169"/>
        <link:link rdf:resource="#item_167"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SimpleNet: A Simple Network for Image Anomaly Detection and Localization</dc:title>
        <dcterms:abstract>We propose a simple and application-friendly network (called SimpleNet) for detecting and localizing anomalies. SimpleNet consists of four components: (1) a pre-trained Feature Extractor that generates local features, (2) a shallow Feature Adapter that transfers local features towards target domain, (3) a simple Anomaly Feature Generator that counterfeits anomaly features by adding Gaussian noise to normal features, and (4) a binary Anomaly Discriminator that distinguishes anomaly features from normal features. During inference, the Anomaly Feature Generator would be discarded. Our approach is based on three intuitions. First, transforming pre-trained features to target-oriented features helps avoid domain bias. Second, generating synthetic anomalies in feature space is more effective, as defects may not have much commonality in the image space. Third, a simple discriminator is much efficient and practical. In spite of simplicity, SimpleNet outperforms previous methods quantitatively and qualitatively. On the MVTec AD benchmark, SimpleNet achieves an anomaly detection AUROC of 99.6%, reducing the error by 55.5% compared to the next best performing model. Furthermore, SimpleNet is faster than existing methods, with a high frame rate of 77 FPS on a 3080ti GPU. Additionally, SimpleNet demonstrates significant improvements in performance on the One-Class Novelty Detection task. Code: https://github.com/DonaldRR/SimpleNet.</dcterms:abstract>
        <dc:date>2023-03-28</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SimpleNet</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2303.15140</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:45:29</dcterms:dateSubmitted>
        <dc:description>arXiv:2303.15140 [cs]</dc:description>
        <prism:number>arXiv:2303.15140</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_169">
       <rdf:value>Comment: Accepted to CVPR 2023</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_167">
        <z:itemType>attachment</z:itemType>
        <dc:title>Liu 等 - 2023 - SimpleNet A Simple Network for Image Anomaly Dete.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2005.02357">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cohen</foaf:surname>
                        <foaf:givenName>Niv</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hoshen</foaf:surname>
                        <foaf:givenName>Yedid</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_170"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Sub-Image Anomaly Detection with Deep Pyramid Correspondences</dc:title>
        <dcterms:abstract>Nearest neighbor (kNN) methods utilizing deep pre-trained features exhibit very strong anomaly detection performance when applied to entire images. A limitation of kNN methods is the lack of segmentation map describing where the anomaly lies inside the image. In this work we present a novel anomaly segmentation approach based on alignment between an anomalous image and a constant number of the similar normal images. Our method, Semantic Pyramid Anomaly Detection (SPADE) uses correspondences based on a multi-resolution feature pyramid. SPADE is shown to achieve state-of-the-art performance on unsupervised anomaly detection and localization while requiring virtually no training time.</dcterms:abstract>
        <dc:date>2021-02-03</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2005.02357</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:45:38</dcterms:dateSubmitted>
        <dc:description>arXiv:2005.02357 [cs]</dc:description>
        <prism:number>arXiv:2005.02357</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_170">
        <z:itemType>attachment</z:itemType>
        <dc:title>Cohen 和 Hoshen - 2021 - Sub-Image Anomaly Detection with Deep Pyramid Corr.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2111.13495">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xiang</foaf:surname>
                        <foaf:givenName>Tiange</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Yixiao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lu</foaf:surname>
                        <foaf:givenName>Yongyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yuille</foaf:surname>
                        <foaf:givenName>Alan L.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Chaoyi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cai</foaf:surname>
                        <foaf:givenName>Weidong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhou</foaf:surname>
                        <foaf:givenName>Zongwei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_174"/>
        <link:link rdf:resource="#item_172"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>SQUID: Deep Feature In-Painting for Unsupervised Anomaly Detection</dc:title>
        <dcterms:abstract>Radiography imaging protocols focus on particular body regions, therefore producing images of great similarity and yielding recurrent anatomical structures across patients. To exploit this structured information, we propose the use of Space-aware Memory Queues for In-painting and Detecting anomalies from radiography images (abbreviated as SQUID). We show that SQUID can taxonomize the ingrained anatomical structures into recurrent patterns; and in the inference, it can identify anomalies (unseen/modiﬁed patterns) in the image. SQUID surpasses 13 state-of-theart methods in unsupervised anomaly detection by at least 5 points on two chest X-ray benchmark datasets measured by the Area Under the Curve (AUC). Additionally, we have created a new dataset (DigitAnatomy), which synthesizes the spatial correlation and consistent shape in chest anatomy. We hope DigitAnatomy can prompt the development, evaluation, and interpretability of anomaly detection methods.</dcterms:abstract>
        <dc:date>2023-03-24</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>SQUID</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2111.13495</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-04 12:46:16</dcterms:dateSubmitted>
        <dc:description>arXiv:2111.13495 [cs]</dc:description>
        <prism:number>arXiv:2111.13495</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_174">
       <rdf:value>Comment: CVPR 2023</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_172">
        <z:itemType>attachment</z:itemType>
        <dc:title>Xiang 等 - 2023 - SQUID Deep Feature In-Painting for Unsupervised A.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_176">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tien</foaf:surname>
                        <foaf:givenName>Tran Dinh</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_175"/>
        <dc:title>Revisiting Reverse Distillation for Anomaly Detection</dc:title>
        <dcterms:abstract>Anomaly detection is an important application in largescale industrial manufacturing. Recent methods for this task have demonstrated excellent accuracy but come with a latency trade-off. Memory based approaches with dominant performances like PatchCore or Coupled-hyperspherebased Feature Adaptation (CFA) require an external memory bank, which significantly lengthens the execution time. Another approach that employs Reversed Distillation (RD) can perform well while maintaining low latency. In this paper, we revisit this idea to improve its performance, establishing a new state-of-the-art benchmark on the challenging MVTec dataset for both anomaly detection and localization. The proposed method, called RD++, runs six times faster than PatchCore, and two times faster than CFA but introduces a negligible latency compared to RD. We also experiment on the BTAD and Retinal OCT datasets to demonstrate our method’s generalizability and conduct important ablation experiments to provide insights into its configurations. Source code will be available at https : / / github . com / tientrandinh / Revisiting-Reverse-Distillation.</dcterms:abstract>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_175">
        <z:itemType>attachment</z:itemType>
        <dc:title>Tien - Revisiting Reverse Distillation for Anomaly Detect.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Data rdf:about="#item_182">
        <z:itemType>computerProgram</z:itemType>
        <dcterms:isReferencedBy rdf:resource="#item_221"/>
        <dcterms:isReferencedBy rdf:resource="#item_410"/>
        <dcterms:isReferencedBy rdf:resource="#item_509"/>
        <dcterms:isReferencedBy rdf:resource="#item_195"/>
        <dcterms:isReferencedBy rdf:resource="#item_487"/>
        <dcterms:isReferencedBy rdf:resource="#item_500"/>
        <dcterms:isReferencedBy rdf:resource="#item_187"/>
        <dcterms:isReferencedBy rdf:resource="#item_400"/>
        <dcterms:isReferencedBy rdf:resource="#item_192"/>
        <dcterms:isReferencedBy rdf:resource="#item_472"/>
        <dcterms:isReferencedBy rdf:resource="#item_245"/>
        <dcterms:isReferencedBy rdf:resource="#item_239"/>
        <dcterms:isReferencedBy rdf:resource="#item_386"/>
        <dcterms:isReferencedBy rdf:resource="#item_486"/>
        <dcterms:isReferencedBy rdf:resource="#item_183"/>
        <dcterms:isReferencedBy rdf:resource="#item_234"/>
        <dcterms:isReferencedBy rdf:resource="#item_447"/>
        <dcterms:isReferencedBy rdf:resource="#item_225"/>
        <dcterms:isReferencedBy rdf:resource="#item_214"/>
        <dcterms:isReferencedBy rdf:resource="#item_184"/>
        <dcterms:isReferencedBy rdf:resource="#item_257"/>
        <dcterms:isReferencedBy rdf:resource="#item_377"/>
        <dcterms:isReferencedBy rdf:resource="#item_215"/>
        <dcterms:isReferencedBy rdf:resource="#item_524"/>
        <dcterms:isReferencedBy rdf:resource="#item_446"/>
        <dcterms:isReferencedBy rdf:resource="#item_508"/>
        <dcterms:isReferencedBy rdf:resource="#item_457"/>
        <dcterms:isReferencedBy rdf:resource="#item_199"/>
        <dcterms:isReferencedBy rdf:resource="#item_411"/>
        <dc:title>Addon Item</dc:title>
    </bib:Data>
    <bib:Memo rdf:about="#item_221">
        <rdf:value>2VTW5H73
{&quot;readingTime&quot;:{&quot;page&quot;:22,&quot;data&quot;:{&quot;0&quot;:100,&quot;6&quot;:10,&quot;10&quot;:10,&quot;17&quot;:10,&quot;19&quot;:40,&quot;20&quot;:250}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_410">
        <rdf:value>3IMM2N5Y
{&quot;readingTime&quot;:{&quot;page&quot;:12,&quot;data&quot;:{&quot;0&quot;:170,&quot;2&quot;:10,&quot;6&quot;:80}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_509">
        <rdf:value>4DSYQ3UF
{&quot;readingTime&quot;:{&quot;page&quot;:7,&quot;data&quot;:{&quot;1&quot;:10,&quot;4&quot;:20}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_195">
        <rdf:value>4NLF8VQ2
{&quot;readingTime&quot;:{&quot;page&quot;:12,&quot;data&quot;:{&quot;0&quot;:400,&quot;1&quot;:1480,&quot;2&quot;:1780,&quot;3&quot;:370,&quot;4&quot;:810,&quot;5&quot;:340,&quot;6&quot;:10,&quot;7&quot;:20,&quot;8&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_487">
        <rdf:value>5E9GFCJ4
{&quot;readingTime&quot;:{&quot;page&quot;:9,&quot;data&quot;:{&quot;0&quot;:10,&quot;1&quot;:10,&quot;2&quot;:10,&quot;5&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_500">
        <rdf:value>7QGBVU4Y
{&quot;readingTime&quot;:{&quot;page&quot;:1397,&quot;data&quot;:{&quot;2&quot;:10,&quot;4&quot;:10,&quot;8&quot;:10,&quot;10&quot;:20,&quot;19&quot;:10,&quot;25&quot;:10,&quot;26&quot;:10,&quot;27&quot;:10,&quot;29&quot;:10,&quot;31&quot;:10,&quot;32&quot;:10,&quot;33&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_187">
        <rdf:value>8LZ3AQQR
{&quot;readingTime&quot;:{&quot;page&quot;:38,&quot;data&quot;:{&quot;0&quot;:140,&quot;1&quot;:210,&quot;2&quot;:20,&quot;3&quot;:750,&quot;4&quot;:2570,&quot;5&quot;:440,&quot;6&quot;:810,&quot;7&quot;:140,&quot;8&quot;:90,&quot;9&quot;:540,&quot;10&quot;:120,&quot;13&quot;:20,&quot;14&quot;:10,&quot;16&quot;:20,&quot;17&quot;:30,&quot;18&quot;:10,&quot;20&quot;:10,&quot;23&quot;:60,&quot;25&quot;:90,&quot;26&quot;:20,&quot;27&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_400">
        <rdf:value>8S2M2YVY
{&quot;readingTime&quot;:{&quot;page&quot;:14,&quot;data&quot;:{&quot;1&quot;:20,&quot;2&quot;:40,&quot;3&quot;:60}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_192">
        <rdf:value>9QPMUDMX
{&quot;readingTime&quot;:{&quot;page&quot;:58,&quot;data&quot;:{&quot;0&quot;:50}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_472">
        <rdf:value>433C5Q2T
{&quot;readingTime&quot;:{&quot;page&quot;:11,&quot;data&quot;:{&quot;0&quot;:10,&quot;2&quot;:50,&quot;3&quot;:90}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_245">
        <rdf:value>CIDGB6NG
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:130,&quot;1&quot;:40,&quot;7&quot;:100,&quot;8&quot;:60}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_239">
        <rdf:value>CNEJLXNM
{&quot;readingTime&quot;:{&quot;page&quot;:11,&quot;data&quot;:{&quot;0&quot;:180,&quot;1&quot;:10,&quot;3&quot;:10,&quot;4&quot;:10,&quot;5&quot;:10,&quot;7&quot;:130}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_386">
        <rdf:value>DB6I7HSK
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;3&quot;:20,&quot;4&quot;:20}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_486">
        <rdf:value>DUYDHENY
{&quot;readingTime&quot;:{&quot;page&quot;:32,&quot;data&quot;:{&quot;0&quot;:560,&quot;1&quot;:150,&quot;2&quot;:560,&quot;3&quot;:790,&quot;4&quot;:100,&quot;5&quot;:20,&quot;6&quot;:10,&quot;8&quot;:100,&quot;10&quot;:30}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_183">
        <rdf:value>GRUKV4SF
{&quot;readingTime&quot;:{&quot;page&quot;:18,&quot;data&quot;:{&quot;0&quot;:30,&quot;2&quot;:10,&quot;5&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_234">
        <rdf:value>HF7VXKBP
{&quot;readingTime&quot;:{&quot;page&quot;:14,&quot;data&quot;:{&quot;0&quot;:140,&quot;7&quot;:10,&quot;8&quot;:10,&quot;9&quot;:60}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_447">
        <rdf:value>HPYJ8SXW
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:370}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_225">
        <rdf:value>HSPMYM88
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:30,&quot;5&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_214">
        <rdf:value>I2TEMCSD
{&quot;readingTime&quot;:{&quot;page&quot;:9,&quot;data&quot;:{&quot;0&quot;:210,&quot;2&quot;:40,&quot;3&quot;:30,&quot;5&quot;:20,&quot;6&quot;:10,&quot;7&quot;:90,&quot;8&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_184">
        <rdf:value>I55DD3QH
{&quot;readingTime&quot;:{&quot;page&quot;:28,&quot;data&quot;:{&quot;0&quot;:510,&quot;1&quot;:60,&quot;2&quot;:10,&quot;3&quot;:10,&quot;6&quot;:80}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_257">
        <rdf:value>LFA5T6MS
{&quot;readingTime&quot;:{&quot;page&quot;:11,&quot;data&quot;:{&quot;0&quot;:10,&quot;2&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_377">
        <rdf:value>MWTEEHMU
{&quot;readingTime&quot;:{&quot;page&quot;:12,&quot;data&quot;:{&quot;0&quot;:10,&quot;3&quot;:20,&quot;11&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_215">
        <rdf:value>PRIGTBQE
{&quot;readingTime&quot;:{&quot;page&quot;:13,&quot;data&quot;:{&quot;0&quot;:150,&quot;2&quot;:30,&quot;3&quot;:10,&quot;4&quot;:10,&quot;6&quot;:10,&quot;8&quot;:70}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_524">
        <rdf:value>RPZHZF6I
{&quot;readingTime&quot;:{&quot;page&quot;:11,&quot;data&quot;:{&quot;0&quot;:80,&quot;3&quot;:10,&quot;5&quot;:10,&quot;6&quot;:20,&quot;7&quot;:10,&quot;8&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_446">
        <rdf:value>TDEXI4B3
{&quot;readingTime&quot;:{&quot;page&quot;:17,&quot;data&quot;:{&quot;0&quot;:80,&quot;1&quot;:10,&quot;4&quot;:20,&quot;8&quot;:10,&quot;9&quot;:10,&quot;13&quot;:40}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_508">
        <rdf:value>WFEHWJUM
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:10,&quot;2&quot;:10,&quot;3&quot;:20,&quot;4&quot;:20,&quot;5&quot;:50}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_457">
        <rdf:value>WVP6A7GK
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:710,&quot;1&quot;:1970,&quot;2&quot;:660,&quot;3&quot;:460,&quot;4&quot;:30,&quot;5&quot;:10,&quot;7&quot;:50,&quot;8&quot;:40,&quot;9&quot;:40}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_199">
        <rdf:value>YPMNXXZ7
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:10,&quot;2&quot;:10}}}</rdf:value>
    </bib:Memo>
    <bib:Memo rdf:about="#item_411">
        <rdf:value>Z7VAI79C
{&quot;readingTime&quot;:{&quot;page&quot;:10,&quot;data&quot;:{&quot;0&quot;:1980,&quot;1&quot;:1630,&quot;2&quot;:3660,&quot;3&quot;:4240,&quot;4&quot;:980,&quot;5&quot;:640,&quot;6&quot;:560,&quot;7&quot;:490,&quot;8&quot;:90}}}</rdf:value>
    </bib:Memo>
    <bib:Article rdf:about="https://dl.acm.org/doi/10.1145/1541880.1541882">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0360-0300,%201557-7341"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chandola</foaf:surname>
                        <foaf:givenName>Varun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Banerjee</foaf:surname>
                        <foaf:givenName>Arindam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kumar</foaf:surname>
                        <foaf:givenName>Vipin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_191"/>
        <dc:title>Anomaly detection: A survey</dc:title>
        <dcterms:abstract>Anomaly detection is an important problem that has been researched within diverse research areas and application domains. Many anomaly detection techniques have been specifically developed for certain application domains, while others are more generic. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. We have grouped existing techniques into different categories based on the underlying approach adopted by each technique. For each category we have identified key assumptions, which are used by the techniques to differentiate between normal and anomalous behavior. When applying a given technique to a particular domain, these assumptions can be used as guidelines to assess the effectiveness of the technique in that domain. For each category, we provide a basic anomaly detection technique, and then show how the different existing techniques in that category are variants of the basic technique. This template provides an easier and more succinct understanding of the techniques belonging to each category. Further, for each category, we identify the advantages and disadvantages of the techniques in that category. We also provide a discussion on the computational complexity of the techniques since it is an important issue in real application domains. We hope that this survey will provide a better understanding of the different directions in which research has been done on this topic, and how techniques developed in one area can be applied in domains for which they were not intended to begin with.</dcterms:abstract>
        <dc:date>07/2009</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Anomaly detection</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/10.1145/1541880.1541882</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-07 11:24:09</dcterms:dateSubmitted>
        <bib:pages>1-58</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0360-0300,%201557-7341">
        <prism:volume>41</prism:volume>
        <dc:title>ACM Computing Surveys</dc:title>
        <dc:identifier>DOI 10.1145/1541880.1541882</dc:identifier>
        <prism:number>3</prism:number>
        <dcterms:alternative>ACM Comput. Surv.</dcterms:alternative>
        <dc:identifier>ISSN 0360-0300, 1557-7341</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_191">
        <z:itemType>attachment</z:itemType>
        <dc:title>Chandola et al_2009_Anomaly detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://dl.acm.org/doi/pdf/10.1145/1541880.1541882</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-07 11:24:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="http://link.springer.com/10.1023/B:MACH.0000008084.60811.49">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0885-6125"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tax</foaf:surname>
                        <foaf:givenName>David M.J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Duin</foaf:surname>
                        <foaf:givenName>Robert P.W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_219"/>
        <dc:title>Support Vector Data Description</dc:title>
        <dcterms:abstract>Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superﬂuous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classiﬁer. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classiﬁer it can be made ﬂexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artiﬁcial and real data.</dcterms:abstract>
        <dc:date>01/2004</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>http://link.springer.com/10.1023/B:MACH.0000008084.60811.49</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 12:05:32</dcterms:dateSubmitted>
        <bib:pages>45-66</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0885-6125">
        <prism:volume>54</prism:volume>
        <dc:title>Machine Learning</dc:title>
        <dc:identifier>DOI 10.1023/B:MACH.0000008084.60811.49</dc:identifier>
        <prism:number>1</prism:number>
        <dcterms:alternative>Machine Learning</dcterms:alternative>
        <dc:identifier>ISSN 0885-6125</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_219">
        <z:itemType>attachment</z:itemType>
        <dc:title>Tax 和 Duin - 2004 - Support Vector Data Description.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2210.07829">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rudolph</foaf:surname>
                        <foaf:givenName>Marco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wehrbein</foaf:surname>
                        <foaf:givenName>Tom</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rosenhahn</foaf:surname>
                        <foaf:givenName>Bodo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wandt</foaf:surname>
                        <foaf:givenName>Bastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_254"/>
        <link:link rdf:resource="#item_256"/>
        <link:link rdf:resource="#item_255"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Asymmetric Student-Teacher Networks for Industrial Anomaly Detection</dc:title>
        <dcterms:abstract>Industrial defect detection is commonly addressed with anomaly detection (AD) methods where no or only incomplete data of potentially occurring defects is available. This work discovers previously unknown problems of student-teacher approaches for AD and proposes a solution, where two neural networks are trained to produce the same output for the defect-free training examples. The core assumption of student-teacher networks is that the distance between the outputs of both networks is larger for anomalies since they are absent in training. However, previous methods suffer from the similarity of student and teacher architecture, such that the distance is undesirably small for anomalies. For this reason, we propose asymmetric student-teacher networks (AST). We train a normalizing flow for density estimation as a teacher and a conventional feed-forward network as a student to trigger large distances for anomalies: The bijectivity of the normalizing flow enforces a divergence of teacher outputs for anomalies compared to normal data. Outside the training distribution the student cannot imitate this divergence due to its fundamentally different architecture. Our AST network compensates for wrongly estimated likelihoods by a normalizing flow, which was alternatively used for anomaly detection in previous work. We show that our method produces state-of-the-art results on the two currently most relevant defect detection datasets MVTec AD and MVTec 3D-AD regarding image-level anomaly detection on RGB and 3D data.</dcterms:abstract>
        <dc:date>2022-10-18</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2210.07829</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 12:50:56</dcterms:dateSubmitted>
        <dc:description>arXiv:2210.07829 [cs]</dc:description>
        <dc:identifier>DOI 10.48550/arXiv.2210.07829</dc:identifier>
        <prism:number>arXiv:2210.07829</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_254">
       <rdf:value>Comment: accepted to WACV 2023</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_256">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2210.07829</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 12:51:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_255">
        <z:itemType>attachment</z:itemType>
        <dc:title>Rudolph et al_2022_Asymmetric Student-Teacher Networks for Industrial Anomaly Detection.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2210.07829.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 12:51:03</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Document rdf:about="https://paperswithcode.com/paper/informative-knowledge-distillation-for-image">
        <z:itemType>webpage</z:itemType>
        <dcterms:isPartOf>
           <z:Website></z:Website>
        </dcterms:isPartOf>
        <dc:title>Papers with Code - Informative knowledge distillation for image anomaly segmentation</dc:title>
        <dcterms:abstract>#19 best model for Anomaly Detection on MVTec AD (Segmentation AUPRO metric)</dcterms:abstract>
        <z:language>en</z:language>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://paperswithcode.com/paper/informative-knowledge-distillation-for-image</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-06-09 12:54:05</dcterms:dateSubmitted>
    </bib:Document>
    <rdf:Description rdf:about="http://arxiv.org/abs/2010.11929">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dosovitskiy</foaf:surname>
                        <foaf:givenName>Alexey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Beyer</foaf:surname>
                        <foaf:givenName>Lucas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kolesnikov</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Weissenborn</foaf:surname>
                        <foaf:givenName>Dirk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhai</foaf:surname>
                        <foaf:givenName>Xiaohua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Unterthiner</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dehghani</foaf:surname>
                        <foaf:givenName>Mostafa</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Minderer</foaf:surname>
                        <foaf:givenName>Matthias</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Heigold</foaf:surname>
                        <foaf:givenName>Georg</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gelly</foaf:surname>
                        <foaf:givenName>Sylvain</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uszkoreit</foaf:surname>
                        <foaf:givenName>Jakob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Houlsby</foaf:surname>
                        <foaf:givenName>Neil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_277"/>
        <link:link rdf:resource="#item_275"/>
        <link:link rdf:resource="#item_274"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</dc:title>
        <dcterms:abstract>While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.</dcterms:abstract>
        <dc:date>2021-06-03</dc:date>
        <z:shortTitle>An Image is Worth 16x16 Words</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2010.11929</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:21</dcterms:dateSubmitted>
        <dc:description>arXiv:2010.11929 [cs]</dc:description>
        <prism:number>arXiv:2010.11929</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_277">
        <rdf:value>Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_275">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2010.11929</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:28</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_274">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2010.11929.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:49</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2111.06377">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Xinlei</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xie</foaf:surname>
                        <foaf:givenName>Saining</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Yanghao</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dollár</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_279"/>
        <link:link rdf:resource="#item_278"/>
        <link:link rdf:resource="#item_276"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Masked Autoencoders Are Scalable Vision Learners</dc:title>
        <dcterms:abstract>This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.</dcterms:abstract>
        <dc:date>2021-12-19</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2111.06377</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:01</dcterms:dateSubmitted>
        <dc:description>arXiv:2111.06377 [cs]</dc:description>
        <prism:number>arXiv:2111.06377</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_279">
        <rdf:value>Comment: Tech report. arXiv v2: add more transfer learning results; v3: add robustness evaluation</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_278">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2111.06377</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:09</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_276">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2111.06377.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:26:24</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1905.04899">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yun</foaf:surname>
                        <foaf:givenName>Sangdoo</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Han</foaf:surname>
                        <foaf:givenName>Dongyoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Oh</foaf:surname>
                        <foaf:givenName>Seong Joon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chun</foaf:surname>
                        <foaf:givenName>Sanghyuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Choe</foaf:surname>
                        <foaf:givenName>Junsuk</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Yoo</foaf:surname>
                        <foaf:givenName>Youngjoon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_282"/>
        <link:link rdf:resource="#item_281"/>
        <link:link rdf:resource="#item_280"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</dc:title>
        <dcterms:abstract>Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .</dcterms:abstract>
        <dc:date>2019-08-07</dc:date>
        <z:shortTitle>CutMix</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1905.04899</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:24:19</dcterms:dateSubmitted>
        <dc:description>arXiv:1905.04899 [cs]</dc:description>
        <prism:number>arXiv:1905.04899</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_282">
        <rdf:value>Comment: Accepted at ICCV 2019 (oral talk). 14 pages, 5 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_281">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1905.04899</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:24:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_280">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1905.04899.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:24:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1612.03144">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lin</foaf:surname>
                        <foaf:givenName>Tsung-Yi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dollár</foaf:surname>
                        <foaf:givenName>Piotr</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Girshick</foaf:surname>
                        <foaf:givenName>Ross</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>He</foaf:surname>
                        <foaf:givenName>Kaiming</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hariharan</foaf:surname>
                        <foaf:givenName>Bharath</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belongie</foaf:surname>
                        <foaf:givenName>Serge</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_284"/>
        <link:link rdf:resource="#item_283"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Feature Pyramid Networks for Object Detection</dc:title>
        <dcterms:abstract>Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But recent deep learning object detectors have avoided pyramid representations, in part because they are compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network (FPN), shows significant improvement as a generic feature extractor in several applications. Using FPN in a basic Faster R-CNN system, our method achieves state-of-the-art single-model results on the COCO detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the COCO 2016 challenge winners. In addition, our method can run at 5 FPS on a GPU and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.</dcterms:abstract>
        <dc:date>2017-04-19</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1612.03144</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:23:49</dcterms:dateSubmitted>
        <dc:description>arXiv:1612.03144 [cs]</dc:description>
        <prism:number>arXiv:1612.03144</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_284">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1612.03144</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:23:57</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_283">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1612.03144.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:23:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1706.03762">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vaswani</foaf:surname>
                        <foaf:givenName>Ashish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shazeer</foaf:surname>
                        <foaf:givenName>Noam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Parmar</foaf:surname>
                        <foaf:givenName>Niki</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Uszkoreit</foaf:surname>
                        <foaf:givenName>Jakob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jones</foaf:surname>
                        <foaf:givenName>Llion</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Gomez</foaf:surname>
                        <foaf:givenName>Aidan N.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kaiser</foaf:surname>
                        <foaf:givenName>Lukasz</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Polosukhin</foaf:surname>
                        <foaf:givenName>Illia</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_286"/>
        <link:link rdf:resource="#item_285"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Computation and Language</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Attention Is All You Need</dc:title>
        <dcterms:abstract>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</dcterms:abstract>
        <dc:date>2017-12-05</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1706.03762</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:22:49</dcterms:dateSubmitted>
        <dc:description>arXiv:1706.03762 [cs]</dc:description>
        <prism:number>arXiv:1706.03762</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_286">
       <rdf:value>Comment: 15 pages, 5 figures</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_285">
        <z:itemType>attachment</z:itemType>
        <dc:title>Vaswani 等 - 2017 - Attention Is All You Need.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1411.4038">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Long</foaf:surname>
                        <foaf:givenName>Jonathan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shelhamer</foaf:surname>
                        <foaf:givenName>Evan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Darrell</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_289"/>
        <link:link rdf:resource="#item_288"/>
        <link:link rdf:resource="#item_287"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Fully Convolutional Networks for Semantic Segmentation</dc:title>
        <dcterms:abstract>Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build &quot;fully convolutional&quot; networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes one third of a second for a typical image.</dcterms:abstract>
        <dc:date>2015-03-08</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1411.4038</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:20:18</dcterms:dateSubmitted>
        <dc:description>arXiv:1411.4038 [cs]</dc:description>
        <prism:number>arXiv:1411.4038</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_289">
       <rdf:value>Comment: to appear in CVPR (2015)</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_288">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1411.4038</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:20:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_287">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1411.4038.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:20:32</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1502.03167">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ioffe</foaf:surname>
                        <foaf:givenName>Sergey</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Szegedy</foaf:surname>
                        <foaf:givenName>Christian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_294"/>
        <link:link rdf:resource="#item_293"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</dc:title>
        <dcterms:abstract>Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error), exceeding the accuracy of human raters.</dcterms:abstract>
        <dc:date>2015-03-02</dc:date>
        <z:shortTitle>Batch Normalization</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1502.03167</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:19</dcterms:dateSubmitted>
        <dc:description>arXiv:1502.03167 [cs]</dc:description>
        <prism:number>arXiv:1502.03167</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_294">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1502.03167</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_293">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1502.03167.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:27</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1409.1556">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Simonyan</foaf:surname>
                        <foaf:givenName>Karen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zisserman</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_296"/>
        <link:link rdf:resource="#item_295"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>VGGnet</dc:subject>
        <dc:title>Very Deep Convolutional Networks for Large-Scale Image Recognition</dc:title>
        <dcterms:abstract>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</dcterms:abstract>
        <dc:date>2015-04-10</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1409.1556</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:18:59</dcterms:dateSubmitted>
        <dc:description>arXiv:1409.1556 [cs]</dc:description>
        <prism:number>arXiv:1409.1556</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_296">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1409.1556</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_295">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1409.1556.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:08</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1406.2661">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Goodfellow</foaf:surname>
                        <foaf:givenName>Ian J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pouget-Abadie</foaf:surname>
                        <foaf:givenName>Jean</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mirza</foaf:surname>
                        <foaf:givenName>Mehdi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Xu</foaf:surname>
                        <foaf:givenName>Bing</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Warde-Farley</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ozair</foaf:surname>
                        <foaf:givenName>Sherjil</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Courville</foaf:surname>
                        <foaf:givenName>Aaron</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bengio</foaf:surname>
                        <foaf:givenName>Yoshua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_298"/>
        <link:link rdf:resource="#item_297"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Generative Adversarial Networks</dc:title>
        <dcterms:abstract>We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.</dcterms:abstract>
        <dc:date>2014-06-10</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1406.2661</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:17:48</dcterms:dateSubmitted>
        <dc:description>arXiv:1406.2661 [cs, stat]</dc:description>
        <prism:number>arXiv:1406.2661</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_298">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1406.2661</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:17:56</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_297">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1406.2661.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:18:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1311.2901">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zeiler</foaf:surname>
                        <foaf:givenName>Matthew D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Fergus</foaf:surname>
                        <foaf:givenName>Rob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_299"/>
        <link:link rdf:resource="#item_292"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Visualizing and Understanding Convolutional Networks</dc:title>
        <dcterms:abstract>Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.</dcterms:abstract>
        <dc:date>2013-11-28</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1311.2901</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:16:42</dcterms:dateSubmitted>
        <dc:description>arXiv:1311.2901 [cs]</dc:description>
        <prism:number>arXiv:1311.2901</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_299">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1311.2901</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:17:50</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_292">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1311.2901.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:19:29</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>25</prism:volume>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Curran Associates, Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krizhevsky</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sutskever</foaf:surname>
                        <foaf:givenName>Ilya</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hinton</foaf:surname>
                        <foaf:givenName>Geoffrey E</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_300"/>
        <dc:title>ImageNet Classification with Deep Convolutional Neural Networks</dc:title>
        <dcterms:abstract>We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7\% and 18.9\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.</dcterms:abstract>
        <dc:date>2012</dc:date>
        <z:libraryCatalog>Neural Information Processing Systems</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:15:13</dcterms:dateSubmitted>
    </rdf:Description>
    <z:Attachment rdf:about="#item_300">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-06 14:15:19</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1603.07285">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dumoulin</foaf:surname>
                        <foaf:givenName>Vincent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Visin</foaf:surname>
                        <foaf:givenName>Francesco</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_305"/>
        <link:link rdf:resource="#item_304"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>A guide to convolution arithmetic for deep learning</dc:title>
        <dcterms:abstract>We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.</dcterms:abstract>
        <dc:date>2018-01-11</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1603.07285</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 07:37:52</dcterms:dateSubmitted>
        <dc:description>arXiv:1603.07285 [cs, stat]</dc:description>
        <prism:number>arXiv:1603.07285</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_305">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1603.07285</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 07:37:59</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_304">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1603.07285.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 07:38:02</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://papers.nips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>31</prism:volume>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Curran Associates, Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kingma</foaf:surname>
                        <foaf:givenName>Durk P</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dhariwal</foaf:surname>
                        <foaf:givenName>Prafulla</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_308"/>
        <dc:title>Glow: Generative Flow with Invertible 1x1 Convolutions</dc:title>
        <dcterms:abstract>Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.</dcterms:abstract>
        <dc:date>2018</dc:date>
        <z:shortTitle>Glow</z:shortTitle>
        <z:libraryCatalog>Neural Information Processing Systems</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://papers.nips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 07:09:34</dcterms:dateSubmitted>
    </rdf:Description>
    <z:Attachment rdf:about="#item_308">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://papers.nips.cc/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 07:09:37</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1811.00220">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Iquebal</foaf:surname>
                        <foaf:givenName>Ashif Sikandar</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bukkapatnam</foaf:surname>
                        <foaf:givenName>Satish</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_310"/>
        <link:link rdf:resource="#item_309"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Electrical Engineering and Systems Science - Image and Video Processing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Consistent estimation of the max-flow problem: Towards unsupervised image segmentation</dc:title>
        <dcterms:abstract>Advances in the image-based diagnostics of complex biological and manufacturing processes have brought unsupervised image segmentation to the forefront of enabling automated, on the fly decision making. However, most existing unsupervised segmentation approaches are either computationally complex or require manual parameter selection (e.g., flow capacities in max-flow/min-cut segmentation). In this work, we present a fully unsupervised segmentation approach using a continuous max-flow formulation over the image domain while optimally estimating the flow parameters from the image characteristics. More specifically, we show that the maximum a posteriori estimate of the image labels can be formulated as a continuous max-flow problem given the flow capacities are known. The flow capacities are then iteratively obtained by employing a novel Markov random field prior over the image domain. We present theoretical results to establish the posterior consistency of the flow capacities. We compare the performance of our approach on two real-world case studies including brain tumor image segmentation and defect identification in additively manufactured components using electron microscopic images. Comparative results with several state-of-the-art supervised as well as unsupervised methods suggest that the present method performs statistically similar to the supervised methods, but results in more than 90% improvement in the Dice score when compared to the state-of-the-art unsupervised methods.</dcterms:abstract>
        <dc:date>2019-06-29</dc:date>
        <z:shortTitle>Consistent estimation of the max-flow problem</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1811.00220</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 06:53:46</dcterms:dateSubmitted>
        <dc:description>arXiv:1811.00220 [cs, eess]</dc:description>
        <prism:number>arXiv:1811.00220</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_310">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1811.00220</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 06:53:54</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_309">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1811.00220.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 06:54:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_334">
        <z:itemType>attachment</z:itemType>
        <dc:title>无人机编队队形保持方案设计(1).pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_335">
        <z:itemType>attachment</z:itemType>
        <dc:title>A202210009055.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_336">
        <z:itemType>attachment</z:itemType>
        <dc:title>B202210011036.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_337">
        <z:itemType>attachment</z:itemType>
        <dc:title>C202210071132.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_338">
        <z:itemType>attachment</z:itemType>
        <dc:title>C202210061036.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_339">
        <z:itemType>attachment</z:itemType>
        <dc:title>B.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_340">
        <z:itemType>attachment</z:itemType>
        <dc:title>A202210065045.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="https://www.nature.com/articles/s41586-021-03819-2">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0028-0836,%201476-4687"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jumper</foaf:surname>
                        <foaf:givenName>John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Evans</foaf:surname>
                        <foaf:givenName>Richard</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pritzel</foaf:surname>
                        <foaf:givenName>Alexander</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Green</foaf:surname>
                        <foaf:givenName>Tim</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Figurnov</foaf:surname>
                        <foaf:givenName>Michael</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ronneberger</foaf:surname>
                        <foaf:givenName>Olaf</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tunyasuvunakool</foaf:surname>
                        <foaf:givenName>Kathryn</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bates</foaf:surname>
                        <foaf:givenName>Russ</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Žídek</foaf:surname>
                        <foaf:givenName>Augustin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Potapenko</foaf:surname>
                        <foaf:givenName>Anna</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bridgland</foaf:surname>
                        <foaf:givenName>Alex</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Meyer</foaf:surname>
                        <foaf:givenName>Clemens</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kohl</foaf:surname>
                        <foaf:givenName>Simon A. A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ballard</foaf:surname>
                        <foaf:givenName>Andrew J.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cowie</foaf:surname>
                        <foaf:givenName>Andrew</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Romera-Paredes</foaf:surname>
                        <foaf:givenName>Bernardino</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Nikolov</foaf:surname>
                        <foaf:givenName>Stanislav</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Jain</foaf:surname>
                        <foaf:givenName>Rishub</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Adler</foaf:surname>
                        <foaf:givenName>Jonas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Back</foaf:surname>
                        <foaf:givenName>Trevor</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Petersen</foaf:surname>
                        <foaf:givenName>Stig</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Reiman</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Clancy</foaf:surname>
                        <foaf:givenName>Ellen</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zielinski</foaf:surname>
                        <foaf:givenName>Michal</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Steinegger</foaf:surname>
                        <foaf:givenName>Martin</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pacholska</foaf:surname>
                        <foaf:givenName>Michalina</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Berghammer</foaf:surname>
                        <foaf:givenName>Tamas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bodenstein</foaf:surname>
                        <foaf:givenName>Sebastian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Silver</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Vinyals</foaf:surname>
                        <foaf:givenName>Oriol</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Senior</foaf:surname>
                        <foaf:givenName>Andrew W.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kavukcuoglu</foaf:surname>
                        <foaf:givenName>Koray</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kohli</foaf:surname>
                        <foaf:givenName>Pushmeet</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Hassabis</foaf:surname>
                        <foaf:givenName>Demis</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_378"/>
        <dc:title>Highly accurate protein structure prediction with AlphaFold</dc:title>
        <dcterms:abstract>Abstract
            
              Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort
              1–4
              , the structures of around 100,000 unique proteins have been determined
              5
              , but this represents a small fraction of the billions of known protein sequences
              6,7
              . Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’
              8
              —has been an important open research problem for more than 50 years
              9
              . Despite recent progress
              10–14
              , existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)
              15
              , demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.</dcterms:abstract>
        <dc:date>2021-08-26</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.nature.com/articles/s41586-021-03819-2</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-11 05:44:52</dcterms:dateSubmitted>
        <bib:pages>583-589</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0028-0836,%201476-4687">
        <prism:volume>596</prism:volume>
        <dc:title>Nature</dc:title>
        <dc:identifier>DOI 10.1038/s41586-021-03819-2</dc:identifier>
        <prism:number>7873</prism:number>
        <dcterms:alternative>Nature</dcterms:alternative>
        <dc:identifier>ISSN 0028-0836, 1476-4687</dc:identifier>
    </bib:Journal>
    <z:Attachment rdf:about="#item_378">
        <z:itemType>attachment</z:itemType>
        <dc:title>Jumper et al. - 2021 - Highly accurate protein structure prediction with .pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1402.1869">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Montúfar</foaf:surname>
                        <foaf:givenName>Guido</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Pascanu</foaf:surname>
                        <foaf:givenName>Razvan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Cho</foaf:surname>
                        <foaf:givenName>Kyunghyun</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bengio</foaf:surname>
                        <foaf:givenName>Yoshua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_382"/>
        <link:link rdf:resource="#item_381"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>On the Number of Linear Regions of Deep Neural Networks</dc:title>
        <dcterms:abstract>We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.</dcterms:abstract>
        <dc:date>2014-06-07</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1402.1869</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 09:16:19</dcterms:dateSubmitted>
        <dc:description>arXiv:1402.1869 [cs, stat]</dc:description>
        <prism:number>arXiv:1402.1869</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_382">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1402.1869</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 09:16:25</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_381">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1402.1869.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-09 09:16:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2211.11317">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Zhang</foaf:surname>
                        <foaf:givenName>Xuan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Shiyu</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Li</foaf:surname>
                        <foaf:givenName>Xi</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Huang</foaf:surname>
                        <foaf:givenName>Ping</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Shan</foaf:surname>
                        <foaf:givenName>Jiulong</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Chen</foaf:surname>
                        <foaf:givenName>Ting</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_392"/>
        <link:link rdf:resource="#item_391"/>
        <link:link rdf:resource="#item_390"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>DeSTSeg: Segmentation Guided Denoising Student-Teacher for Anomaly Detection</dc:title>
        <dcterms:abstract>Visual anomaly detection, an important problem in computer vision, is usually formulated as a one-class classification and segmentation task. The student-teacher (S-T) framework has proved to be effective in solving this challenge. However, previous works based on S-T only empirically applied constraints on normal data and fused multi-level information. In this study, we propose an improved model called DeSTSeg, which integrates a pre-trained teacher network, a denoising student encoder-decoder, and a segmentation network into one framework. First, to strengthen the constraints on anomalous data, we introduce a denoising procedure that allows the student network to learn more robust representations. From synthetically corrupted normal images, we train the student network to match the teacher network feature of the same images without corruption. Second, to fuse the multi-level S-T features adaptively, we train a segmentation network with rich supervision from synthetic anomaly masks, achieving a substantial performance improvement. Experiments on the industrial inspection benchmark dataset demonstrate that our method achieves state-of-the-art performance, 98.6% on image-level AUC, 75.8% on pixel-level average precision, and 76.4% on instance-level average precision.</dcterms:abstract>
        <dc:date>2023-03-21</dc:date>
        <z:shortTitle>DeSTSeg</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2211.11317</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-13 11:13:28</dcterms:dateSubmitted>
        <dc:description>arXiv:2211.11317 [cs]</dc:description>
        <prism:number>arXiv:2211.11317</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_392">
       <rdf:value>Comment: Accepted by CVPR 2023</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_391">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2211.11317</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-13 11:13:39</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_390">
        <z:itemType>attachment</z:itemType>
        <dc:title>Full Text PDF</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2211.11317.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-13 11:13:53</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="https://proceedings.neurips.cc/paper_files/paper/2016/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <prism:volume>29</prism:volume>
                <dc:title>Advances in Neural Information Processing Systems</dc:title>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
               <foaf:name>Curran Associates, Inc.</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Veit</foaf:surname>
                        <foaf:givenName>Andreas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wilber</foaf:surname>
                        <foaf:givenName>Michael J</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Belongie</foaf:surname>
                        <foaf:givenName>Serge</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_408"/>
        <dc:title>Residual Networks Behave Like Ensembles of Relatively Shallow Networks</dc:title>
        <dcterms:abstract>In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.</dcterms:abstract>
        <dc:date>2016</dc:date>
        <z:libraryCatalog>Neural Information Processing Systems</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper_files/paper/2016/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-18 01:39:27</dcterms:dateSubmitted>
    </rdf:Description>
    <z:Attachment rdf:about="#item_408">
        <z:itemType>attachment</z:itemType>
        <dc:title>Veit et al_2016_Residual Networks Behave Like Ensembles of Relatively Shallow Networks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://proceedings.neurips.cc/paper_files/paper/2016/file/37bc2f75bf1bcfe8450a1a41c200364c-Paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-18 01:39:44</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/2&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/3&quot;&gt;The unraveled view of residual networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/4&quot;&gt;Lesion study&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/5&quot;&gt;Experiment: Deleting individual layers from neural networks at test time&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/5&quot;&gt;Experiment: Deleting many modules from residual networks at test-time&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/6&quot;&gt;Experiment: Reordering modules in residual networks at test-time&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/6&quot;&gt;The importance of short paths in residual networks&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/7&quot;&gt;Discussion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_VR8GT4FK/8&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <bib:Article rdf:about="http://arxiv.org/abs/1908.09257">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf rdf:resource="urn:issn:0162-8828,%202160-9292,%201939-3539"/>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kobyzev</foaf:surname>
                        <foaf:givenName>Ivan</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Prince</foaf:surname>
                        <foaf:givenName>Simon J. D.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brubaker</foaf:surname>
                        <foaf:givenName>Marcus A.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_439"/>
        <link:link rdf:resource="#item_440"/>
        <link:link rdf:resource="#item_444"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Normalizing Flows: An Introduction and Review of Current Methods</dc:title>
        <dcterms:abstract>Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.</dcterms:abstract>
        <dc:date>2021-11-1</dc:date>
        <z:shortTitle>Normalizing Flows</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1908.09257</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:21:49</dcterms:dateSubmitted>
        <dc:description>arXiv:1908.09257 [cs, stat]</dc:description>
        <bib:pages>3964-3979</bib:pages>
    </bib:Article>
    <bib:Journal rdf:about="urn:issn:0162-8828,%202160-9292,%201939-3539">
        <prism:volume>43</prism:volume>
        <dc:title>IEEE Transactions on Pattern Analysis and Machine Intelligence</dc:title>
        <dc:identifier>DOI 10.1109/TPAMI.2020.2992934</dc:identifier>
        <prism:number>11</prism:number>
        <dcterms:alternative>IEEE Trans. Pattern Anal. Mach. Intell.</dcterms:alternative>
        <dc:identifier>ISSN 0162-8828, 2160-9292, 1939-3539</dc:identifier>
    </bib:Journal>
    <bib:Memo rdf:about="#item_439">
        <rdf:value>Comment: This paper appears in: IEEE Transactions on Pattern Analysis and Machine Intelligence On page(s): 1-16 Print ISSN: 0162-8828 Online ISSN: 0162-8828</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_440">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1908.09257</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:22:01</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_444">
        <z:itemType>attachment</z:itemType>
        <dc:title>Kobyzev et al_2021_Normalizing Flows.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1908.09257.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:25:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/1&quot;&gt;2 Background&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/2&quot;&gt;2.1 Basics&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/2&quot;&gt;2.1.1 More formal construction&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/3&quot;&gt;2.2 Applications&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/3&quot;&gt;2.2.1 Density estimation and sampling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/3&quot;&gt;2.2.2 Variational Inference&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/3&quot;&gt;3 Methods&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/3&quot;&gt;3.1 Elementwise Flows&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/4&quot;&gt;3.2 Linear Flows&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/4&quot;&gt;3.2.1 Diagonal&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/4&quot;&gt;3.2.2 Triangular&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/4&quot;&gt;3.2.3 Permutation and Orthogonal&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/4&quot;&gt;3.2.4 Factorizations&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.2.5 Convolution&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.3 Planar and Radial Flows&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.3.1 Planar Flows&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.3.2 Radial Flows&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.4 Coupling and Autoregressive Flows&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/5&quot;&gt;3.4.1 Coupling Flows&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/6&quot;&gt;3.4.2 Autoregressive Flows&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/7&quot;&gt;3.4.3 Universality&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/7&quot;&gt;3.4.4 Coupling Functions&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/9&quot;&gt;3.5 Residual Flows&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/9&quot;&gt;3.6 Infinitesimal (Continuous) Flows&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/10&quot;&gt;3.6.1 ODE-based methods&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/10&quot;&gt;3.6.2 SDE-based methods (Langevin flows)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/11&quot;&gt;4 Datasets and performance&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/11&quot;&gt;4.1 Tabular datasets&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;4.2 Image datasets&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5 Discussion and open problems&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.1 Inductive biases&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.1.1 Role of the base measure&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.1.2 Form of diffeomorphisms&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.1.3 Loss function&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.2 Generalisation to non-Euclidean spaces&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:24px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/12&quot;&gt;5.2.1 Flows on manifolds.&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/13&quot;&gt;5.2.2 Discrete distributions&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/17&quot;&gt;Biographies&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/17&quot;&gt;Ivan Kobyzev&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/17&quot;&gt;Simon J.D. Prince&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_8Z46GURZ/17&quot;&gt;Marcus A. Brubaker&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1505.05770">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rezende</foaf:surname>
                        <foaf:givenName>Danilo Jimenez</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mohamed</foaf:surname>
                        <foaf:givenName>Shakir</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_442"/>
        <link:link rdf:resource="#item_443"/>
        <link:link rdf:resource="#item_445"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Computation</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Methodology</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Variational Inference with Normalizing Flows</dc:title>
        <dcterms:abstract>The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.</dcterms:abstract>
        <dc:date>2016-06-14</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1505.05770</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:24:27</dcterms:dateSubmitted>
        <dc:description>arXiv:1505.05770 [cs, stat]</dc:description>
        <prism:number>arXiv:1505.05770</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_442">
        <rdf:value>Comment: Proceedings of the 32nd International Conference on Machine Learning</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_443">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1505.05770</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:24:36</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_445">
        <z:itemType>attachment</z:itemType>
        <dc:title>Rezende_Mohamed_2016_Variational Inference with Normalizing Flows.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1505.05770.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-07-24 08:26:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Memo rdf:about="#item_468">
        <z:itemType>note</z:itemType>
        <rdf:value>&lt;div data-schema-version=&quot;8&quot;&gt;&lt;h1&gt;Distribution Map&lt;/h1&gt;
&lt;h2&gt;Flow模型&lt;/h2&gt;
&lt;h3&gt;DifferNet&lt;/h3&gt;
&lt;p&gt;利用标准化流（Normalizing Flow），将一类正常样本映射到标准正态分布这个隐空间上，利用标准化流的双射性质找到这类样本的分布。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;主要面向于缺陷检测，而不是定位&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;zotero://note/u/7U5FMUG9/?ignore=1&quot; rel=&quot;noopener noreferrer nofollow&quot;&gt;文章笔记&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;文章通过建立正常图片特征向量所在分布与标准正态分布之间的流模型双射关系，将输入图片特征向量变换到一个标准正态分布的隐空间上，根据输入图片的特征向量在隐空间上的概率密度值，确定该图片的异常分数。对于异常图片，通过该图片在隐空间上的特征向量，在模型网络上反向传播得到梯度图，再加以高斯模糊，即可进行异常定位。&lt;/p&gt;
&lt;p&gt;文章创新点：是第一个将流模型应用到缺陷检测中的方法，利用标准化流找将一类正常样本的特征向量映射到标准正态分布上，以找出这类正常样本的分布&lt;/p&gt;
&lt;p&gt;Referred in &lt;a href=&quot;zotero://note/u/XBWUJK7D/?ignore=1&amp;amp;line=5&quot; rel=&quot;noopener noreferrer nofollow&quot;&gt;Distribution Map&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;</rdf:value>
    </bib:Memo>
    <rdf:Description rdf:about="http://arxiv.org/abs/1907.02392">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Ardizzone</foaf:surname>
                        <foaf:givenName>Lynton</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Lüth</foaf:surname>
                        <foaf:givenName>Carsten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Kruse</foaf:surname>
                        <foaf:givenName>Jakob</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Rother</foaf:surname>
                        <foaf:givenName>Carsten</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Köthe</foaf:surname>
                        <foaf:givenName>Ullrich</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_471"/>
        <link:link rdf:resource="#item_470"/>
        <dc:relation rdf:resource="urn:isbn:978-1-66540-477-8"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>68T01</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Guided Image Generation with Conditional Invertible Neural Networks</dc:title>
        <dcterms:abstract>In this work, we address the task of natural image generation guided by a conditioning input. We introduce a new architecture called conditional invertible neural network (cINN). The cINN combines the purely generative INN model with an unconstrained feed-forward network, which efficiently preprocesses the conditioning input into useful features. All parameters of the cINN are jointly optimized with a stable, maximum likelihood-based training procedure. By construction, the cINN does not experience mode collapse and generates diverse samples, in contrast to e.g. cGANs. At the same time our model produces sharp images since no reconstruction loss is required, in contrast to e.g. VAEs. We demonstrate these properties for the tasks of MNIST digit generation and image colorization. Furthermore, we take advantage of our bi-directional cINN architecture to explore and manipulate emergent properties of the latent space, such as changing the image style in an intuitive way.</dcterms:abstract>
        <dc:date>2019-07-04</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1907.02392</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-01 08:05:13</dcterms:dateSubmitted>
        <dc:description>arXiv:1907.02392 [cs]
version: 1</dc:description>
        <prism:number>arXiv:1907.02392</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_471">
        <z:itemType>attachment</z:itemType>
        <dc:title>Ardizzone et al_2019_Guided Image Generation with Conditional Invertible Neural Networks.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1907.02392v1.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-01 08:05:34</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_470">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1907.02392</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-01 08:05:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1410.8516">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dinh</foaf:surname>
                        <foaf:givenName>Laurent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Krueger</foaf:surname>
                        <foaf:givenName>David</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bengio</foaf:surname>
                        <foaf:givenName>Yoshua</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_478"/>
        <link:link rdf:resource="#item_479"/>
        <link:link rdf:resource="#item_480"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>NICE: Non-linear Independent Components Estimation</dc:title>
        <dcterms:abstract>We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.</dcterms:abstract>
        <dc:date>2015-04-10</dc:date>
        <z:shortTitle>NICE</z:shortTitle>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1410.8516</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:53:56</dcterms:dateSubmitted>
        <dc:description>arXiv:1410.8516 [cs]</dc:description>
        <prism:number>arXiv:1410.8516</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_478">
        <rdf:value>Comment: 11 pages and 2 pages Appendix, workshop paper at ICLR 2015</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_479">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1410.8516</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:54:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_480">
        <z:itemType>attachment</z:itemType>
        <dc:title>Dinh et al_2015_NICE.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1410.8516.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:54:11</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/1&quot;&gt;1 Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/2&quot;&gt;2 Learning bijective transformations of continuous probabilities&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/3&quot;&gt;3 Architecture&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/3&quot;&gt;3.1 Triangular structure&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/3&quot;&gt;3.2 Coupling layer&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/4&quot;&gt;3.3 Allowing rescaling&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/5&quot;&gt;3.4 Prior distribution&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/5&quot;&gt;4 Related methods&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/6&quot;&gt;5 Experiments&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/6&quot;&gt;5.1 Log-likelihood and generation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/9&quot;&gt;5.2 Inpainting&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/9&quot;&gt;6 Conclusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/12&quot;&gt;A Further Visualizations&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/12&quot;&gt;A.1 Manifold visualization&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/12&quot;&gt;A.2 Spectrum&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/12&quot;&gt;B Approximate whitening&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_ZK672GU2/12&quot;&gt;C Variational auto-encoder as NICE&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/1605.08803">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Dinh</foaf:surname>
                        <foaf:givenName>Laurent</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Sohl-Dickstein</foaf:surname>
                        <foaf:givenName>Jascha</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Bengio</foaf:surname>
                        <foaf:givenName>Samy</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <dcterms:isReferencedBy rdf:resource="#item_483"/>
        <link:link rdf:resource="#item_484"/>
        <link:link rdf:resource="#item_485"/>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Artificial Intelligence</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Computer Science - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Neural and Evolutionary Computing</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:subject>
            <z:AutomaticTag>
               <rdf:value>Statistics - Machine Learning</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Density estimation using Real NVP</dc:title>
        <dcterms:abstract>Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.</dcterms:abstract>
        <dc:date>2017-02-27</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/1605.08803</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:55:00</dcterms:dateSubmitted>
        <dc:description>arXiv:1605.08803 [cs, stat]</dc:description>
        <prism:number>arXiv:1605.08803</prism:number>
    </rdf:Description>
    <bib:Memo rdf:about="#item_483">
        <rdf:value>Comment: 10 pages of main content, 3 pages of bibliography, 18 pages of appendix. Accepted at ICLR 2017</rdf:value>
    </bib:Memo>
    <z:Attachment rdf:about="#item_484">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/1605.08803</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:55:07</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_485">
        <z:itemType>attachment</z:itemType>
        <dc:title>Dinh et al_2017_Density estimation using Real NVP.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/1605.08803.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-02 02:55:22</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Article rdf:about="#item_489">
        <z:itemType>journalArticle</z:itemType>
        <dcterms:isPartOf>
           <bib:Journal></bib:Journal>
        </dcterms:isPartOf>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Platt</foaf:surname>
                        <foaf:givenName>Communicated John</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Haykin</foaf:surname>
                        <foaf:givenName>Simon</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_488"/>
        <dc:subject>change of variable formula</dc:subject>
        <dc:title>An Information-Maximization Approach to Blind Separation and Blind Deconvolution</dc:title>
        <dc:date>1995</dc:date>
        <z:language>en</z:language>
        <z:libraryCatalog>Zotero</z:libraryCatalog>
    </bib:Article>
    <z:Attachment rdf:about="#item_488">
        <z:itemType>attachment</z:itemType>
        <dc:title>Platt 和 Haykin - An Information-Maximization Approach to Blind Sepa.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <bib:Book rdf:about="urn:isbn:978-1-285-74155-0">
        <z:itemType>book</z:itemType>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>Boston, MA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>Cengage Learning</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Stewart</foaf:surname>
                        <foaf:givenName>James</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_498"/>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Calculus</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:subject>
           <z:AutomaticTag><rdf:value>Textbooks</rdf:value></z:AutomaticTag>
        </dc:subject>
        <dc:title>Calculus: early transcendentals</dc:title>
        <dc:date>2016</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>Calculus</z:shortTitle>
        <z:libraryCatalog>BnF ISBN</z:libraryCatalog>
        <dc:subject>
           <dcterms:LCC><rdf:value>QA303.2 .S7315 2016</rdf:value></dcterms:LCC>
        </dc:subject>
        <dc:identifier>ISBN 978-1-285-74155-0</dc:identifier>
        <prism:edition>Eighth edition</prism:edition>
        <z:numPages>1</z:numPages>
    </bib:Book>
    <z:Attachment rdf:about="#item_498">
        <z:itemType>attachment</z:itemType>
        <dc:title>Stewart - 2016 - Calculus early transcendentals.pdf</dc:title>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="urn:isbn:978-1-66548-739-9">
        <z:itemType>conferencePaper</z:itemType>
        <dcterms:isPartOf>
            <bib:Journal>
                <dc:identifier>ISBN 978-1-66548-739-9</dc:identifier>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</dc:title>
                <dc:identifier>DOI 10.1109/CVPRW56347.2022.00080</dc:identifier>
            </bib:Journal>
        </dcterms:isPartOf>
        <dc:publisher>
            <foaf:Organization>
                <vcard:adr>
                    <vcard:Address>
                       <vcard:locality>New Orleans, LA, USA</vcard:locality>
                    </vcard:Address>
                </vcard:adr>
                <foaf:name>IEEE</foaf:name>
            </foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Wyatt</foaf:surname>
                        <foaf:givenName>Julian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Leach</foaf:surname>
                        <foaf:givenName>Adam</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Schmon</foaf:surname>
                        <foaf:givenName>Sebastian M.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Willcocks</foaf:surname>
                        <foaf:givenName>Chris G.</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_526"/>
        <dc:title>AnoDDPM: Anomaly Detection with Denoising Diffusion Probabilistic Models using Simplex Noise</dc:title>
        <dcterms:abstract>Generative models have been shown to provide a powerful mechanism for anomaly detection by learning to model healthy or normal reference data which can subsequently be used as a baseline for scoring anomalies. In this work we consider denoising diffusion probabilistic models (DDPMs) for unsupervised anomaly detection. DDPMs have superior mode coverage over generative adversarial networks (GANs) and higher sample quality than variational autoencoders (VAEs). However, this comes at the expense of poor scalability and increased sampling times due to the long Markov chain sequences required. We observe that within reconstruction-based anomaly detection a full-length Markov chain diffusion is not required. This leads us to develop a novel partial diffusion anomaly detection strategy that scales to high-resolution imagery, named AnoDDPM. A secondary problem is that Gaussian diffusion fails to capture larger anomalies; therefore we develop a multi-scale simplex noise diffusion process that gives control over the target anomaly size. AnoDDPM with simplex noise is shown to significantly outperform both f-AnoGAN and Gaussian diffusion for the tumorous dataset of 22 T1weighted MRI scans (CCBS Edinburgh) qualitatively and quantitatively (improvement of +25.5% Sørensen–Dice coefficient, +17.6% IoU and +7.4% AUC).</dcterms:abstract>
        <dc:date>6/2022</dc:date>
        <z:language>en</z:language>
        <z:shortTitle>AnoDDPM</z:shortTitle>
        <z:libraryCatalog>DOI.org (Crossref)</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://ieeexplore.ieee.org/document/9857019/</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:31:09</dcterms:dateSubmitted>
        <bib:pages>649-655</bib:pages>
        <bib:presentedAt>
            <bib:Conference>
                <dc:title>2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</dc:title>
            </bib:Conference>
        </bib:presentedAt>
    </rdf:Description>
    <z:Attachment rdf:about="#item_526">
        <z:itemType>attachment</z:itemType>
        <dc:title>Wyatt 等 - 2022 - AnoDDPM Anomaly Detection with Denoising Diffusio.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/papers/Wyatt_AnoDDPM_Anomaly_Detection_With_Denoising_Diffusion_Probabilistic_Models_Using_Simplex_CVPRW_2022_paper.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:31:04</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_528">
        <z:itemType>attachment</z:itemType>
        <dc:title>Anomaly Detection using Diffusion Model without Diffusion</dc:title>
        <dc:identifier>
            <dcterms:URI>
                <rdf:value>https://www.jstage.jst.go.jp/article/pjsai/JSAI2022/0/JSAI2022_1F5GS1001/_pdf/-char/ja</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:34:38</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
    </z:Attachment>
    <rdf:Description rdf:about="http://arxiv.org/abs/2305.15956">
        <z:itemType>preprint</z:itemType>
        <dc:publisher>
           <foaf:Organization><foaf:name>arXiv</foaf:name></foaf:Organization>
        </dc:publisher>
        <bib:authors>
            <rdf:Seq>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Mousakhan</foaf:surname>
                        <foaf:givenName>Arian</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Brox</foaf:surname>
                        <foaf:givenName>Thomas</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
                <rdf:li>
                    <foaf:Person>
                        <foaf:surname>Tayyub</foaf:surname>
                        <foaf:givenName>Jawad</foaf:givenName>
                    </foaf:Person>
                </rdf:li>
            </rdf:Seq>
        </bib:authors>
        <link:link rdf:resource="#item_530"/>
        <link:link rdf:resource="#item_531"/>
        <dc:subject>
            <z:AutomaticTag>
                <rdf:value>Computer Science - Computer Vision and Pattern Recognition</rdf:value>
            </z:AutomaticTag>
        </dc:subject>
        <dc:title>Anomaly Detection with Conditioned Denoising Diffusion Models</dc:title>
        <dcterms:abstract>Reconstruction-based methods have struggled to achieve competitive performance on anomaly detection. In this paper, we introduce Denoising Diffusion Anomaly Detection (DDAD). We propose a novel denoising process for image reconstruction conditioned on a target image. This results in a coherent restoration that closely resembles the target image. Subsequently, our anomaly detection framework leverages this conditioning where the target image is set as the input image to guide the denoising process, leading to defectless reconstruction while maintaining nominal patterns. We localise anomalies via a pixel-wise and feature-wise comparison of the input and reconstructed image. Finally, to enhance the effectiveness of feature comparison, we introduce a domain adaptation method that utilises generated examples from our conditioned denoising process to fine-tune the feature extractor. The veracity of the approach is demonstrated on various datasets including MVTec and VisA benchmarks, achieving state-of-the-art results of 99.5% and 99.3% image-level AUROC respectively.</dcterms:abstract>
        <dc:date>2023-05-25</dc:date>
        <z:libraryCatalog>arXiv.org</z:libraryCatalog>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>http://arxiv.org/abs/2305.15956</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:34:51</dcterms:dateSubmitted>
        <dc:description>arXiv:2305.15956 [cs]</dc:description>
        <prism:number>arXiv:2305.15956</prism:number>
    </rdf:Description>
    <z:Attachment rdf:about="#item_530">
        <z:itemType>attachment</z:itemType>
        <dc:title>arXiv.org Snapshot</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/abs/2305.15956</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:34:58</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>text/html</link:type>
    </z:Attachment>
    <z:Attachment rdf:about="#item_531">
        <z:itemType>attachment</z:itemType>
        <dc:title>Mousakhan et al_2023_Anomaly Detection with Conditioned Denoising Diffusion Models.pdf</dc:title>
        <dc:identifier>
            <dcterms:URI>
               <rdf:value>https://arxiv.org/pdf/2305.15956.pdf</rdf:value>
            </dcterms:URI>
        </dc:identifier>
        <dcterms:dateSubmitted>2023-08-04 01:35:06</dcterms:dateSubmitted>
        <z:linkMode>1</z:linkMode>
        <link:type>application/pdf</link:type>
        <dc:description>&lt;p xmlns=&quot;http://www.w3.org/1999/xhtml&quot; id=&quot;title&quot;&gt;&lt;strong&gt;Contents&lt;/strong&gt;&lt;/p&gt;&lt;ul xmlns=&quot;http://www.w3.org/1999/xhtml&quot; style=&quot;list-style-type: none; padding-left:0px&quot; id=&quot;toc&quot;&gt;&lt;li&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/1&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/2&quot;&gt;Related Work&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/3&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/4&quot;&gt;Denoising Diffusion Anomaly Detection&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/4&quot;&gt;Conditioned Denoising Process for Reconstruction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/5&quot;&gt;Reconstruction for Anomaly Detection&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/5&quot;&gt;Anomaly Scoring&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/5&quot;&gt;Domain Adaptation&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/6&quot;&gt;Experiments&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/6&quot;&gt;Datasets and Evaluation Metrics&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/6&quot;&gt;Experimental Setting&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/7&quot;&gt;Experimental Results and Discussions&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/8&quot;&gt;Inference Time&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/9&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/13&quot;&gt;Implementation Details&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/13&quot;&gt;Additional Quantitative results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/13&quot;&gt;Dependency on conditioning and comparison metrics on MVTec&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/13&quot;&gt;Detailed results on the PRO metric&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/14&quot;&gt;Influence of different denoising steps&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:8px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/14&quot;&gt;Additional Qualitative results&lt;/a&gt;&lt;ul style=&quot;list-style-type: none; padding-left:12px&quot;&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/14&quot;&gt;Effect of conditioning on reconstruction&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/14&quot;&gt;Robustness to anomalies on the background&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/15&quot;&gt;Mislocalisation&lt;/a&gt;&lt;/li&gt;&lt;li style=&quot;padding-top:4px&quot;&gt;&lt;a href=&quot;zotero://open-pdf/0_C8HF63NC/15&quot;&gt;Qualitative results on MTD&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;</dc:description>
    </z:Attachment>
    <z:Collection rdf:about="#collection_17">
       <dc:title>数模</dc:title><dcterms:hasPart rdf:resource="#collection_18"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_18">
        <dc:title>2022reference</dc:title>
        <dcterms:hasPart rdf:resource="#item_334"/>
        <dcterms:hasPart rdf:resource="#item_335"/>
        <dcterms:hasPart rdf:resource="#item_336"/>
        <dcterms:hasPart rdf:resource="#item_337"/>
        <dcterms:hasPart rdf:resource="#item_338"/>
        <dcterms:hasPart rdf:resource="#item_339"/>
        <dcterms:hasPart rdf:resource="#item_340"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_5">
        <dc:title>CV</dc:title>
        <dcterms:hasPart rdf:resource="#collection_1"/>
        <dcterms:hasPart rdf:resource="#collection_6"/>
        <dcterms:hasPart rdf:resource="#collection_13"/>
        <dcterms:hasPart rdf:resource="#collection_16"/>
        <dcterms:hasPart rdf:resource="https://papers.nips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_1">
        <dc:title>AnomalyDetection</dc:title>
        <dcterms:hasPart rdf:resource="#collection_2"/>
        <dcterms:hasPart rdf:resource="#collection_19"/>
        <dcterms:hasPart rdf:resource="#collection_11"/>
        <dcterms:hasPart rdf:resource="#collection_3"/>
        <dcterms:hasPart rdf:resource="#collection_12"/>
        <dcterms:hasPart rdf:resource="#collection_10"/>
        <dcterms:hasPart rdf:resource="#collection_4"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_2">
        <dc:title>Detection</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1805.10917"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1904.02639"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1812.04606"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1901.08954"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9157105/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.02359"/>
        <dcterms:hasPart rdf:resource="#item_90"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2007.08176"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1805.06725"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1812.02288"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1802.06222"/>
        <dcterms:hasPart rdf:resource="#item_113"/>
        <dcterms:hasPart rdf:resource="#item_150"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-0-7695-3502-9"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_19">
        <dc:title>DistributionMap</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.14140"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66540-477-8"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9706997/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2107.12571"/>
        <dcterms:hasPart rdf:resource="#item_468"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_11">
        <dc:title>KnowledgeDistillation</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2103.04257"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66544-509-2"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66546-946-3"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72817-168-5"/>
        <dcterms:hasPart rdf:resource="#item_75"/>
        <dcterms:hasPart rdf:resource="#item_176"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2210.07829"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_3">
        <dc:title>Localization</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2106.03844"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2103.04257"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66544-509-2"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9577693/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2104.04015"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2108.07610"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.14140"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2011.08785"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66540-477-8"/>
        <dcterms:hasPart rdf:resource="https://ojs.aaai.org/index.php/AAAI/article/view/19915"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2203.00259"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2110.04538"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66546-946-3"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9880272/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2106.08265"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2104.13897"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66540-915-5"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9706997/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2107.12571"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-3-030-69543-9%20978-3-030-69544-6"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-72817-168-5"/>
        <dcterms:hasPart rdf:resource="#item_75"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2105.14737"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2111.07677"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9879727/"/>
        <dcterms:hasPart rdf:resource="#item_158"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2303.15140"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2005.02357"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2111.13495"/>
        <dcterms:hasPart rdf:resource="#item_176"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2210.07829"/>
        <dcterms:hasPart rdf:resource="https://paperswithcode.com/paper/informative-knowledge-distillation-for-image"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2211.11317"/>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-66548-739-9"/>
        <dcterms:hasPart rdf:resource="#item_528"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2305.15956"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_12">
        <dc:title>OCC</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2106.03844"/>
        <dcterms:hasPart rdf:resource="https://ieeexplore.ieee.org/document/9577693/"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2104.04015"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_10">
       <dc:title>Reconstruct</dc:title>
    </z:Collection>
    <z:Collection rdf:about="#collection_4">
        <dc:title>Survey</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2301.11514"/>
        <dcterms:hasPart rdf:resource="https://dl.acm.org/doi/10.1145/1541880.1541882"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_6">
        <dc:title>ImageProcessing</dc:title>
        <dcterms:hasPart rdf:resource="http://ieeexplore.ieee.org/document/1284395/"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_13">
        <dc:title>OutlierDetection</dc:title>
        <dcterms:hasPart rdf:resource="http://link.springer.com/10.1023/B:MACH.0000008084.60811.49"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_16">
        <dc:title>Segmentation</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1811.00220"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_7">
        <dc:title>DataMining</dc:title>
        <dcterms:hasPart rdf:resource="https://scienceopen.com/hosted-document?doi=10.14236/ewic/VOCS2008.18"/>
        <dcterms:hasPart rdf:resource="https://linkinghub.elsevier.com/retrieve/pii/S1877050920320524"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_8">
        <dc:title>DeepLearning</dc:title>
        <dcterms:hasPart rdf:resource="#collection_15"/>
        <dcterms:hasPart rdf:resource="#collection_20"/>
        <dcterms:hasPart rdf:resource="#collection_14"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1512.03385"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1603.05027"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2010.11929"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2111.06377"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1905.04899"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1612.03144"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1706.03762"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1411.4038"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1502.03167"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1409.1556"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1406.2661"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1311.2901"/>
        <dcterms:hasPart rdf:resource="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"/>
        <dcterms:hasPart rdf:resource="https://www.nature.com/articles/s41586-021-03819-2"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1402.1869"/>
        <dcterms:hasPart rdf:resource="#item_489"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_15">
        <dc:title>CNN</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1512.03385"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1411.4038"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1409.1556"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1311.2901"/>
        <dcterms:hasPart rdf:resource="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1603.07285"/>
        <dcterms:hasPart rdf:resource="https://proceedings.neurips.cc/paper_files/paper/2016/hash/37bc2f75bf1bcfe8450a1a41c200364c-Abstract.html"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_20">
        <dc:title>Flow</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1908.09257"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1505.05770"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1907.02392"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1410.8516"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1605.08803"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_14">
        <dc:title>Transformer</dc:title>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/2010.11929"/>
        <dcterms:hasPart rdf:resource="http://arxiv.org/abs/1706.03762"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_21">
        <dc:title>E-books</dc:title>
        <dcterms:hasPart rdf:resource="#collection_22"/>
    </z:Collection>
    <z:Collection rdf:about="#collection_22">
        <dc:title>Math</dc:title>
        <dcterms:hasPart rdf:resource="urn:isbn:978-1-285-74155-0"/>
    </z:Collection>
</rdf:RDF>
